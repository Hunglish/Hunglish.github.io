<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>无所住而生其心</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-07-14T18:00:24.854Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>余洋</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>真正意义上的货币战争</title>
    <link href="http://yoursite.com/2018/07/14/20180714%E7%9C%9F%E6%AD%A3%E6%84%8F%E4%B9%89%E7%9A%84%E8%B4%A7%E5%B8%81%E6%88%98%E4%BA%89/"/>
    <id>http://yoursite.com/2018/07/14/20180714真正意义的货币战争/</id>
    <published>2018-07-13T17:04:32.000Z</published>
    <updated>2018-07-14T18:00:24.854Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章写于16年中旬，彼时股市正处于暴跌之中，而从美元指数已经进入到了增强周期。而今人民币在没有编制基础的官媒中阴跌了两年，截止到本文截止时已到了6.73，破7的言论甚嚣尘上。以铜为鉴，可以正衣冠；以史为鉴，可以知兴替。在此危急存亡之际，再次从技术的角度梳理一下当年索罗斯做空泰铢步骤，以为后事之师。</p><p>中国政府已明确提出要<strong>加快实现人民币资本项目可兑换</strong>。汲取近期 A 股震荡的教训， 认真做好国外金融危机的案例分析，梳理货币攻击的各种做法，<strong>搞清楚每个交易或市场开放的风险，弄懂吃透而不是想当然，开放才能够更加心中有数。无疑，从技术角度梳理泰铢狙击战和港币保卫战的路线，对于我们开放资本账户后有效应对资本流动冲击，维护国家金融安全具有重要借鉴意义</strong>。</p><h3 id="货币攻击：技术角度的梳理"><a href="#货币攻击：技术角度的梳理" class="headerlink" title="货币攻击：技术角度的梳理"></a>货币攻击：技术角度的梳理</h3><p>货币攻击的历史可以追溯到 19 世纪末和 20 世纪初金本位时期。当时全球资本高度流动，如果某国传出政治经济的负面消息，很容易引发投资者对其黄金储备进行攻击。只是由于金本位制度相对稳定，调节机制也较为完善，加之各国全力干预，才压制了投机者的攻击。1980 年代以来，随着金融全球化的加快发展、交易技术的不断进步以及衍生品市场的日益繁荣，货币攻击呈现出新的特征。 </p><p>当代货币攻击是立体攻击。国际炒家在<font color="navy"><strong>货币、 外汇、股票和金融衍生品</strong></font>市场同时对一种货币发动进攻，使固定汇率制度崩溃，而炒家则从金融动荡中牟取暴利。当代货币攻击主要沿以下路线展开：</p><h4 id="在即期市场上卖空本币"><a href="#在即期市场上卖空本币" class="headerlink" title="在即期市场上卖空本币"></a>在即期市场上卖空本币</h4><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180714rate01.png" alt=""></p><p>国际炒家<font color=""><strong>先借入目标国的本币</strong></font>。 本币来源主要包括目标国的<strong>货币市场、国际金融市场(离岸市场)及抛售的该国股票或者债券</strong>。国际炒家会采取渐近的方式逐步吸入，以防止短期借入大量目标国本币导致其持有本币的成本上升。直到其手中握有足够多的本币，再对本币进行集中抛售，打压本币汇率(<strong>这一句话看似简单，但是它所需要的技术手段非常高超，由于弹药不可能完全完备，需要合理的利用好群众的情绪</strong>)。炒家还会注意抛售时机的选择，一般会在目标国经济金融的负面消息传出之时对其本币进行抛售；或者散布负面消息的谣言，带动其他投资者进行效仿。一旦目标国本币贬值，炒家就以低价购回本币进行偿还，本币汇率价差扣除借款所需利息就是炒家的利润。上述过程中，<font color="red"><strong>炒家能否获取目标国本币是攻击的关键</strong></font>。如果非居民能够更容易地在目标国货币市场上获得本币；或者目标国本币国际化的程度较高，非居民更容易从离岸金融市场上获取本币；抑或非居民能够更方便地持有及出售目标国的股票、债券，实施该种攻击就会更容易。 </p><h4 id="在远期市场看空本币"><a href="#在远期市场看空本币" class="headerlink" title="在远期市场看空本币"></a>在远期市场看空本币</h4><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180714rate02.png" alt=""></p><p>如图 2 所示，国际炒家在远期外汇市场上出售所攻击货币的远期合约，如果远期合约到期之日所攻击的货币贬值，<strong>通过交割合约国际炒家就可获利（做空）。 远期抛售攻击目标国的本币，将使其本币远期汇率下跌，同时，也会对其即期汇率形成打压</strong>：一方面，银行预计到未来要大量买入本币，就会在即期外汇市场上出售本币换取美元来轧平头寸，以便届时履约（如果不理解这一点的人，可以去了解一下轧平头寸是什么意思）；另一方面，投资者观察到远期本币贬值，也会在即期外汇市场上借入本币，再通过抛售本币兑换成美元，实现其在远期合约到期之日用较少的美元换回履约所需本币的套利目标。银行和投资者抛售本币的行为都会导致本币即期汇率贬值。这种进攻方式对国际炒家而言可谓高风险、高收益。<strong>从远期合约签订日到履约日之间，炒家无需真正交易就能形成对本币的攻击，成本极低；但如果到期日本币没有贬值反而升值，炒家则会面临损失的风险。因此，在操作手法上，炒家会逐步订立大量履约期不同的远期合约</strong>。这样做尽管不能确保每笔合约都获利，但只要本币汇率大方向是朝贬值的方向变动，炒家就可以在总体上获利。此外，在签订远期外汇合约的同时，国际炒家还可以利用目标国本币外汇期货合约的空头，买入本币看跌期权等，待本币贬值之时交割合约获利。 </p><h4 id="三市联动构成对冲"><a href="#三市联动构成对冲" class="headerlink" title="三市联动构成对冲"></a>三市联动构成对冲</h4><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180714rate03.png" alt=""></p><p>如图 3 所示，<strong>国际炒家在目标国即期外汇市场上卖空目标国本币的同时，还可卖空股票，积累股指期货的空头头寸。目标国央行为了限制炒家获取本币，一般会采取提高短期利率的方式来抬升炒家的资金成本。利率上升会对股市形成打压。一旦股市下跌，炒家就可以通过低价购回股票和交割股指期货来获利</strong>。</p><pre><code>真是妙啊</code></pre><font color="navy"><strong>在实际操作中，国际炒家往往是以上两种或者三种方法一起使用。这样的立体攻击方式可以充分利用利率、汇率、股票、股指期货之间的联动关系，来保证赚取高收益</strong></font>。<br><br>- 如果被攻击的货币贬值，炒家可以在外汇市场获利；<br>- 如果被攻击的货币没有贬值，由于在这一过程中目标国央行为保卫汇率，会推高短期利率而对股市形成打压，炒家则可以从股市上获利。<br><br>当然，现实中炒家的操作手法可能更加复杂。 但有证据表明，国际炒家在发动攻击时确曾使用基于上述三种路线的攻击方式。<font color="navy"><strong>索罗斯在其著作中曾谈到：“如果你把一般的投资组合看成是扁平或者二维的，我们的投资组合则更像建筑物。我们建立一个三维的空间，用基本股票作为抵押来扩大杠杆。 我们用 1000 美元至少可以买进 5 万美元的长期债券。我们卖空股票或者债券，即借入股票或者债券待其价格下跌时再买入。我们也操作外汇或者股指的头寸，多空都有。这样创造出一个由风险和获利机会组成的立体结构”</strong>。</font> <p>一般认为，应对炒家的货币攻击有三道防卫措施，即外汇干预、提高利率以及交易管制：<font color=""><strong>货币当局在即期和远期外汇市场购进本币，可以稳定汇率；提高利率可以抬升炒家借入本币的成本；交易管制可以直接限制炒家的攻击手段。但是，每种措施又有各自的局限性</strong></font>：</p><ul><li>外汇干预，受限于外汇储备的数量；</li><li>提高利率，除可能会给炒家提供进行攻击的机会外，还会对经济体内部产生负面影响，如果财政和金融系统本身脆弱，则可能引发国内金融危机；</li><li>管制措施，在实施过程中的效果具有不确定性，有可能加剧市场恐慌。</li></ul><p>面对上述种种挑战，如果被攻击的经济体又缺乏应对经验，一旦应对不当，即使基本面较为健康也会陷入货币危机。 </p><h3 id="泰铢狙击战"><a href="#泰铢狙击战" class="headerlink" title="泰铢狙击战"></a>泰铢狙击战</h3><p>亚洲金融危机中，泰国之所以成为国际投机资本率先攻击的对象，主要根源在于其自身。在 1997 年泰国货币危机爆发的前 10 年里，<strong>泰国经济高速增长的背后潜藏着过度依赖外贸、贸易逆差过大等结构性问题</strong>。</p><p>开放资本账户后，资本大量流入催生了股市和楼市泡沫，并加剧了信贷扩张。跨境借款几乎不受限，造成短期外债过高。由于泰铢对美元汇率保持稳定，1996 年美元升值带动泰铢升值，同时日元发生了贬值，都重创泰国出口，造成该国经济下滑。经济外部失衡、资产价格泡沫、金融部门脆弱、基本面负面冲击，给国际炒家以可乘之机。政局动荡、政府频繁更替也削弱了泰国应对危机的能力。 </p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180714rate04.png" alt=""></p><p>国际炒家早就嗅到攻击的机会。他们的惯用手法是，一旦发现不可持续的资产价格泡沫，就卖空估值过高的有关资产或者货币，以使其贬值，进而牟取暴利。如果市场恐慌情绪和投资者的悲观心理 形成后引发“羊群效应”，他们的火力就足以对被攻击对象形成猛烈冲击。索罗斯和他旗下的“量子基金”是对冲基金的重要代表，1992 年 9 月做空英镑令其一战成名，被《经济学人》杂志称作“战胜了英格兰银行的人”。此后，固定汇率也成为倍受对冲基金“青睐”的攻击目标。 </p><h4 id="火力侦查"><a href="#火力侦查" class="headerlink" title="火力侦查"></a>火力侦查</h4><p>狙击泰铢是预谋已久的。索罗斯在其著作中承认，他的基金公司至少提前 6 个月就已预见到亚洲金融危机。<font color="navy"><strong>在 1995 年 1 月中旬泰国的房地产价格开始下跌时，对冲基金就对泰铢进行了试探性进攻——在即期外汇市场大量抛售泰铢</strong>。</font>由于泰国央行入市干预下，此次进攻并未酿成危机。<font color="navy"><strong>当时墨西哥危机刚刚发生，包括世界银行和国际货币基金组织(IMF)在内的各方面，都对泰国具有信心，认为除了贸易逆差偏大以外，泰国经济比墨西哥要健康得多，并不具备发生货币危机的条件</strong></font>。</p><p>虽然首战遇挫，但投机资本没有放弃，“量子基金”的情报部门通过各种渠道一直在搜集情报，对泰国经济金融方面的信息进行分析。索罗斯本人则坐镇后方，一边积极存入保证金、囤积货币，一边在市场上散布泰铢即将贬值的消息，吸引了大量投机资本蠢蠢欲动。随着泰国经济下行，资产价格泡沫破裂，金融部门问题显现，国际投机资本开始展开大规模进攻。 </p><h4 id="短兵相接"><a href="#短兵相接" class="headerlink" title="短兵相接"></a>短兵相接</h4><p>1997 年 2 月，以“量子基金”为代表的国际投机资本大量做空泰铢，借入泰铢(包括通过曼谷国际银行便利〔BIBF〕借入泰铢)并抛售。2 月 14 日，泰铢汇率跌至 10 年来最低点的 1 美元兑 26.18 泰铢。 <font color="navy"><strong>泰国央行进行了坚决反击，在外汇市场上大量购入泰铢，同时提高短期利率，使投机资本的资金成本大幅提高</strong></font>。在这两项措施的作用下，泰铢即期汇率很快得到稳定，泰国央行暂时打退了国际投机资本的攻击。<font color=""><strong>但泰国方面也付出了代价：一方面，外汇储备被大量消耗；另一方面，高利率对国内经济的负面影响逐步显现，银行和企业的坏账问题开始暴露</strong></font>。国际炒家此役虽然遇挫，但他们由此断定，泰国政府会死守固定汇率却实力不足，从而坚定了攻击的决心。关于事后广为诟病的死守固定汇率的问题，事实上早在 1996 年 4 月泰国央行就开始考虑放弃固定汇率，只是未付诸实施。而此时，再放弃固定汇率似乎为时已晚：由于外债过高，如果泰铢贬值，必然使企业的负债升值而资产贬值，许多企业马上会变得资不抵债，进而导致银行坏账攀升甚至引发银行危机，泰铢贬值的宏观经济后果难以预计。基于上述因素，再加上政局动荡，央行和财政部的负责人怕担责任，导致泰国在放弃固定汇率的问题上一直举棋不定。 </p><h4 id="胶着阶段"><a href="#胶着阶段" class="headerlink" title="胶着阶段"></a>胶着阶段</h4><p>国际炒家进一步逼近，把战场延伸到远期市场。早在 1997 年初，国际炒家就开始了大规模的买美元卖泰铢的远期外汇交易，“明修栈道暗度陈仓”，分阶段抛空远期泰铢。而泰国对此还全然不知，仍在大量提供远期合约。同年 2、3 月份，银行间市场上类似的远期外汇合约需求量激增，高达 150 亿 美元。此举引发投资者纷纷效仿。到了 5 月中，国际炒家又开始在即期市场上大量抛售泰铢，至5月 底，泰铢已下跌至1美元兑 26.6 泰铢的低点。此时，泰国央行才开始反击：一是干预远期市场，大量卖出远期美元、买入泰铢；二是联合新加坡、香港和马来西亚货币当局干预即期市场，耗资100亿美元购入泰铢；三是严禁国内银行拆借泰铢给国际炒家；四是大幅提高隔夜拆借利率。此外，泰国政府还采取许多非常手段，包括威逼、利诱泰国的银行提供远期外汇合约的客户资料；扬言要打击刊登不利消息的媒体，并出动警察追踪发布负面新闻的人。但这一切为时已晚，泰铢已经落入炒家布好的圈套。 </p><p>国际炒家针锋相对，在 6 月份继续出售美国国债筹集资金对泰铢进行最后的扑杀，同时散布泰国已经黔驴技穷的消息。一些外资银行开始在报纸上刊登广告，表示可以帮助投资者将外汇汇出泰国；泰国国内的贸易商也开始做出安排，加快将泰铢兑换成美元，加速了泰国外汇储备的消耗。 </p><h4 id="失去抵抗"><a href="#失去抵抗" class="headerlink" title="失去抵抗"></a>失去抵抗</h4><p>泰国政府一直坚守固定汇率，却没有更好的办法来反击国际炒家。经过几轮交锋，泰国的外汇储备消耗殆尽：至1997年6月，泰国央行的外汇储备仅剩下60亿—70亿美元。虽然在6月19日泰国总理仍坚称泰铢绝不贬值，但随后泰国财政部长的辞职，则加剧了市场的恐慌情绪。6 月 28 日，泰国外汇储备进一步减少到 28 亿美元，干预能力几近枯竭，完全失去了抵抗。当局被迫于 7 月 2 日宣布放弃固定汇率，导致泰铢暴跌。7 月 28 日，泰国向IMF发出救援请求。泰铢贬值标志着东南亚货币危机全面爆发。此役，国际炒家大获全胜，并携得胜之威横扫东南亚：菲律宾比索、印尼盾和马来西亚林吉特相继贬值，新加坡也受到冲击。泰铢贬值最终演变成席卷全球新兴市场的亚洲金融危机。 </p><h3 id="港币保卫战"><a href="#港币保卫战" class="headerlink" title="港币保卫战"></a>港币保卫战</h3><p>与泰国在经济金融方面存在许多问题不同，香港的经济基本面更加健康。从整体上看，香港有着完善的市场经济制度，经济富有活力。1997 年香港实际 GDP 增速达到 5.1%，财政盈余占 GDP 的比重 为 6.3%，通货膨胀虽然较高，但已经连续三年下降。</p><p>金融方面，香港的银行体系经过 1980 年代的危机，监管制度不断完善，资本充足率提高，银行业综合实力在东南亚经济体中位居前列。香港的联系汇率制度自 1983 年施行以来运行稳定，在国际上享有良好的信誉。联系汇率通过市场套利机制将港币与美元挂钩，汇率固定在 1 美元兑 7.8 港币，在香港这样一个小型开放经济体稳定金融方面发挥了重要作用。此外，香港还拥有 1000 亿美元的外汇储备，当时位居全球第三。</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180714rate05.png" alt=""></p><p>国际炒家为什么仍然敢对香港下手？</p><p><strong>一是香港房地产市场和股票市场泡沫十分明显，为国际投机资本提供了攻击目标</strong>。房地产行业是香港经济的支柱产业，1997年房地产业对 GDP 的贡献率高达26.8%。由于香港在 1990 年代以来通货膨胀率较高，<strong>居民往往用房产作为对冲通货膨胀的手段</strong>。外资的大量涌入也助推香港房价上涨，1995年10月到1997年10月，香港的房价平均上涨了 80%。股市方面，恒生指数从1995初到1997年8月两年半时间里上涨了1.4倍。</p><p>二是联系汇率仍存在缺陷。在联系汇率制度下，虽然现钞的发行是以相应的美元作保证的，但港币存款则没有保证。例如，截至1997 年三季度，香港广义货币达到 28000 亿港币。虽然有近千亿美元外汇储备，但是一旦香港民众的信心发生动摇，要求把港币兑换成美元，联系汇率也难以守住。此外，当时国际炒家连续攻击东南亚 固定汇率经济体得手，<strong>特别是经济基本面较好的新加坡和中国台湾地区也放纵汇率贬值，壮大了投机资本的声势</strong>。</p><h4 id="交锋"><a href="#交锋" class="headerlink" title="交锋"></a>交锋</h4><p>从 1997 年 7 月开始，投机资本对港币发动了多次立体攻击。香港的银行间市场隔夜拆借利率在8月 19日一度达到 10%，此后居高不下，一直徘徊 在 6%—7%之间。香港股市则一路下行。国际炒家第一次大规模进攻发生在 10 月 20 日当周。国际炒家先在货币市场大量抛售港币，导致港币汇率下跌。 银行把获得的港币卖给香港金管局，市场上港币的流动性收紧。到了 10 月 23 日清算日，许多银行在金管局结算账户上的港币已经没有足够结余(注: 按照香港的全额即时结算体制，所有银行不得在其结算账户上隔日透支)，而金管局为了提高国际炒家借港币的成本，不仅没有注入港币流动性，反而发出通知要对反复通过流动性机制向金管局借港币的银行收取惩罚性的高息。一时间银行间市场港币难求，同业隔夜拆借利率一度飙升至 300%。 </p><p>虽然国际炒家没有在汇率上得手，但当天恒生指数从 11700 点暴跌至 10600 点，下跌 10.4%，国际炒家从股市下跌中获利颇丰。香港股市大跌引发全球股市连锁反应，10 月 27 日，道琼斯指数大跌 554点，创史上最大跌幅；巴西、墨西哥等新兴市场股市也大跌。28 日，恒生指数再度下跌 13.7%，收报 9060 点。 </p><p>事后，香港金管局对保卫港币的行动进行了调查和反思，并提出了一项旨在强化市场规则和透明度的 30 点计划，为 1998 年直接入市干预打下基础。 </p><h4 id="绝地反击"><a href="#绝地反击" class="headerlink" title="绝地反击"></a>绝地反击</h4><p>临近 1997 年年底，危机的阴云在亚洲上空笼罩。1997年11月，日本、韩国经济相继陷入衰退。11月21日，韩国向IMF发出求助。进入1998年，国际炒家继续不断对港币发动攻击。1 月初，印尼卢比暴跌之后，港币遭受明显抛售压力。1月21日，香港最大的投行百富勤爆出丑闻倒闭，当日恒生指数下挫 8.7%，报收 8121 点。1998 年 6 月中旬，国际炒家再次对港币发动攻击。8 月初，日元大幅贬值让人们对亚洲货币的信心降到冰点。1998年7月底和 8 月初，有媒体文章称，人民币将贬值，许多机构因此预测香港联系汇率制度将要崩溃。 </p><p>1998年8月，港币到了最危险的时候。国际炒家的攻击变本加厉：第一，大肆卖空港币。从1998 年年初到 8 月中旬，每当港币利率稳定时就借入港币，到 8 月，港币空头估计达到 300 亿港币以上。第二，大量积累股票和股指期货的空头头寸。香港当局估计，截至 1998 年 8 月，国际炒家大约有8万份空头合约。恒生指数每下跌 1000 点，国际炒家便可获利 40 亿港币。第三，等待时机随时准备抛售港币，推高利率、打压股市，然后通过股指期货空头获利。 </p><p>8月5日，即期外汇市场出现 300 多亿港币的卖盘，远期外汇市场也出现 116 亿港币的卖盘；8月6日，香港和伦敦市场又出现 155 亿港币的卖盘；8月7日，市场再出现 78 亿港币的卖盘。同日，恒生指数收报 7018.41 点，下跌 3.5%，5 个交易日累计跌去 917 点，2500 亿港币市值化为乌有。面对来 势汹汹的卖盘，香港金管局一改以往不直接干预的做法，动用外汇储备直接入市买入港币。这使得在维持港币汇率稳定的同时，银行间利率也没有像往常一样上升。当日，香港特首董建华发表谈话表示，“维系联系汇率是特区政府坚定不移的政策”。国际炒家在汇市遭遇顽强抵抗后，在股市继续发动攻击：他们大量抛售蓝筹股，继续建立股指期货空头头寸。8月13日，恒生指数跌至 6660 点。</p><p>8月14日，港府毫无征兆地突然入市干预，发起了绝地反击。</p><p>第一，购买恒生指数中 33 种成分股，拉动指数攀升，当天恒生指数上升 564 点，报收 7244 点，炒家受到初步打击。</p><p>第二，在远期外汇市场上承接国际炒家的卖盘。</p><p>第三，要求各券商不要向国际炒家借出股票，同时监管当局向托管银行和信托机构借入股票，切断炒家的“弹药”供应。</p><p>第四，在股指期货市场展开进攻。8月24日，不少对冲基金因卢布贬值遭受损失，急于从香港市场套现。港府将计就计，推高8月期指，逼炒家平仓；同时，拉低 9月份期指。由于 9 月份期指比 8 月份低 130 点，炒家转仓会有 100 多点损失，这对炒家形成夹击之势。第五，继续采用推高利率的方法，增加炒家成本。8 月 28 日，港府与国际炒家的较量迎来决战，港府坚决买入股票、国际炒家大肆抛售，当日股市交易量突破 790 亿元港币，高出历史最高成交额 70%。恒生指数最终站稳 7829 点。</p><p>9 月，港府出台了完善联系汇率制度的 7 项技术性措施(即著名的“任七招”)和维护证券市场稳定的 30 点措施，进一步巩固了战果。主要包括动用外汇储备来维护汇率和利率稳定，同时严格金融市场的交易规则，遏制投机行为。<strong>“任七招”中最主要的措施是金管局承诺持牌银行可以将其结余的港币，按 1 美元兑 7.75 港币的固定汇率向金管局兑换美元；同时，以贴现窗口取代流动资金调节机制。贴现窗口的基本利率由金管局确定，并根据实际情况进行调整</strong>。这大大完善了联系汇率制度，避免了港币汇率和利率大幅波动。“30 点措施”主要包括：</p><ul><li><p>限制抛空港币。股票和股指期货交割期限由 14 天缩短为 2 天，使得抛空头寸必须在 2 天内回补。</p></li><li><p>降低股指期货杠杆作用。对持有1万个长期或短期股指期货合同的投资者征收 150% 的特别保证金。</p></li><li><p>完善交易报告制度。把需要呈报的持有大量股指期货合同的最低数量由 500 单位降为 250 单位，以便监管机构能够充分了解炒家情况。 </p></li></ul><p>这些措施压缩了炒家的操作空间，同时逼炒家现形</p><h4 id="战果"><a href="#战果" class="headerlink" title="战果"></a>战果</h4><p>港府的反击经过精心策划，坚决而有力。随着市场信心恢复，国际炒家眼看战斗机会已经消逝，只好悻悻退去。从双方来看，港府此役花费了 150 亿美元左右，但是随着股市回升，购入的股票有相当的盈利。最重要的是，入市干预成功保住了联系汇率，挽救了市场对香港的信心危机，使香港金融中心的地位进一步巩固。国际炒家因在前期股市下跌过程中利用做空股票和股指期货获利较大，因而在随后的争夺战中虽然败下阵来，但总体上仍有可能是获利的。 （都他妈的获利，说到底就是这韭菜友邦不能割的太过分）</p><h3 id="启示"><a href="#启示" class="headerlink" title="启示"></a>启示</h3><h4 id="金融市场开放无小事"><a href="#金融市场开放无小事" class="headerlink" title="金融市场开放无小事"></a>金融市场开放无小事</h4><p>因为成功实施出口导向型经济发展战略，1990 年初，泰国崛起成为亚洲“四小虎”。然而，当泰国雄心勃勃地为争取成为区域国际金融中心，加快金融开放步伐时，却因市场制度不完善、经济基本面状况恶化，招致了货币攻击。扩大对外开放并不必然会倒逼出必要的对内改革和调整，而且由于金融市场超调的特性，金融开放与贸易开放对实体经济的影响也不可以简单类比外推。前者的影响具有高度的不确定性，因此金融开放需要大胆设想、小心求证。资本账户开放不可孤立进行，需要一系列改革协同推进。在这方面，泰国显然缺乏足够的准备。</p><h4 id="货币攻击都是从资本流入开始的"><a href="#货币攻击都是从资本流入开始的" class="headerlink" title="货币攻击都是从资本流入开始的"></a>货币攻击都是从资本流入开始的</h4><p>一方面，前期大量资本流入，尤其是短期资本流入，积累了经济金融的脆弱性；另一方面，投机者发起货币攻击，都要通过在岸或离岸市场获得目标国的本币资产，才能够做空本币。<strong>所以，防止资本流动冲击的工作，应该始于资本流入之时。特别是当经济繁荣时，会对国际资本产生巨大的吸引力，而这也往往会埋下未来资本集中流出的隐患</strong>。如果对外开放的金融市场存在诸多扭曲，则有可能被投资者蓄意利用，加倍放大，加大东道国的金融脆弱性。对此，必须居安思危、防患未然。<strong>同时，一旦遭受货币攻击，增加投机者获取本币的成本或者限制其获取本币的能力是重要的阻击手段</strong>。在这方面，马来西亚最后通过限制境外林吉特回流实施外汇管制，以及香港及时出台的30点措施限制投机，均产生了明显的效果。 </p><h4 id="固定汇率制度安排易受到冲击"><a href="#固定汇率制度安排易受到冲击" class="headerlink" title="固定汇率制度安排易受到冲击"></a>固定汇率制度安排易受到冲击</h4><p>一系列原因导致在资本自由流动条件下固定汇率制度的内在不稳定性。不仅会由于经济基本面恶化引致的过度扩张的宏观经济政策，最终导致固定汇率制度崩溃(第一代货币危机模型)，而且会受私人部门预期的影响，使经济由好的均衡转向坏的均衡，政府由支持固定汇率转向放弃固定汇率，引发预期自我实现的多重均衡危机(第二代货币危机模型)，还会由于政府对企业和金融机构的隐性担保引发道德风险，造成过度投机性投资和资产价格泡沫，并因泡沫破裂和资本外逃导致危机(第三代货币危机模型)。泰国是基本面问题演变成流动性危机，香港则是传染效应的多重均衡危机。<strong>无论怎么解释货币攻击发生的原因，固定汇率的直接结果往往就是汇率高估，随后成为国际炒家的攻击目标</strong>。 汇率高估会增大市场对本币的贬值预期，为国际炒家做空本币提供便利条件。有研究表明，在危机爆发前，泰铢和港币都表现出高估。<strong>固定汇率在经济发展初期发挥了重要作用，但随着金融市场化改革的推进，在条件具备的情况下增大汇率弹性以防止汇率高估，则是避免被攻击的有效手段</strong>。</p><h4 id="充足的外汇储备是捍卫货币的重要但非根本保障"><a href="#充足的外汇储备是捍卫货币的重要但非根本保障" class="headerlink" title="充足的外汇储备是捍卫货币的重要但非根本保障"></a>充足的外汇储备是捍卫货币的重要但非根本保障</h4><p>外汇储备越多，货币当局在外汇市场维护本币汇率的能力越强。但是，不能自恃外汇储备体量大就放松对货币攻击的警惕。理论上，即使一国的外 汇储备能够应付外债和进口支付，一旦居民信心发 生动摇，争相把本币兑换成外币，再多外汇储备也可能耗尽。特别是短期资本流入形成的外汇储备，更不能作为应对货币攻击的屏障，因为一旦形势发生逆转，这部分外汇储备会首先被消耗掉。另外，尽管泰国和香港外汇储备都曾经比较充裕，但投机者有针对性的策略仍使之沦为“自动提款机”。这表明，高额外汇储备并不能对投机者起到绝对的阻遏和威慑作用。 </p><h4 id="政府正确施策是应对货币攻击的关键"><a href="#政府正确施策是应对货币攻击的关键" class="headerlink" title="政府正确施策是应对货币攻击的关键"></a>政府正确施策是应对货币攻击的关键</h4><p>国际炒家在外汇市场展开大规模进攻，即使未形成垄断、也已占据市场的主导地位。此时，政府干预正是要维护公平和竞争的自由经济原则。但要成功应对货币攻击，需注意以下几个方面：一是多方协调应对攻击。货币攻击往往是立体化攻击，要仔细分析外汇市场、股票市场、衍生品市场之间的联动性，正确运用政策组合进行应对。香港的经验是在汇市、股市和期市联合进行反击。当时香港还成立了跨市场趋势监察小组，负责证券及期货运作的联合交易所、期货交易所、中央结算公司、证监会、新成立的财经事务局以及香港金管局，都派代表加入到小组中，以共同密切监控市场形势、交换看法，对炒家的攻击行为进行预判并研究制定应对措施。二是完善市场制度。在宏观手段受限的情况下，从微观制度安排入手来抵挡国际炒家的攻击是合乎逻辑且有效的。香港“任七招”推出后，明显增强了联系汇率制度的稳健性，防止了资本外逃造成本币贬值。应完善金融市场交易制度，特别是完善交易报告制度，做到知己知彼，不打无准备之仗。 </p><p>泰国交易报告制度不完善，政府无法对炒家的进攻进行准确分析，最后被洗劫一空。坊间甚至有传闻，如果不是 6 月份泰国财长辞职助长了炒家气焰，7 月初再坚持几天到交割日，境外炒家就爆仓了。而保卫港币成功的关键，就在于了解对手底牌。港府在反击之前就已经对炒家的资金布局和攻击策略进行了摸底，每天需要多少资金干预都心中有数，在反击时能做到有的放矢和精准出击。三是底部的干预才能够增加胜算。“非常之时当用非常之策”。在全面评估形势后，港府出手十分果断，动用外汇储备和养老基金大量买入权重股，立竿见影，迅速稳定住了股市，稳定了投资者心理预期，为港币保卫战取得最终胜利发挥了关键性的作用。最终股市上涨，盈富基金获得了可观的利润，也减轻了港府救市成本。</p><p>希望我天朝能够顺利度过此劫。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇文章写于16年中旬，彼时股市正处于暴跌之中，而从美元指数已经进入到了增强周期。而今人民币在没有编制基础的官媒中阴跌了两年，截止到本文截止时已到了6.73，破7的言论甚嚣尘上。以铜为鉴，可以正衣冠；以史为鉴，可以知兴替。在此危急存亡之际，再次从技术的角度梳理一下当年索罗斯
      
    
    </summary>
    
      <category term="Muse" scheme="http://yoursite.com/categories/Muse/"/>
    
    
      <category term="ecnomics" scheme="http://yoursite.com/tags/ecnomics/"/>
    
  </entry>
  
  <entry>
    <title>热度TopN排名算法设计沉思录</title>
    <link href="http://yoursite.com/2018/07/08/20180708%E7%83%AD%E5%BA%A6TopN%E6%8E%92%E5%90%8D%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    <id>http://yoursite.com/2018/07/08/20180708热度TopN排名算法设计沉思录/</id>
    <published>2018-07-08T03:09:15.000Z</published>
    <updated>2018-07-08T03:11:58.609Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>特征选择与稀疏学习</title>
    <link href="http://yoursite.com/2018/06/25/20180625%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/06/25/20180625特征选择与稀疏学习/</id>
    <published>2018-06-25T10:24:01.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>特征选择是我们进行机器学习训练前必须要考虑的一个步骤，周志华说特征选择是避免维度灾难和去重冗余特征，但是这两点都是为避免学习复杂度。但是也不能盲目去掉冗余特征，要根据结果考虑。如果计算体积的话，长宽底面积高，我们可以去掉长宽，保留底面积。</p><p>特征选择的过程本质上讲，离不开两个步骤：</p><ul><li>子集搜索</li><li>子集评价</li></ul><p>子集搜索一般都是贪心和穷举法，而子集评价的方法非常多，比较好用的一种就是计算信息增益。它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。对一个特征而言，系统有它和没它时信息量将发生变化，而前后信息量的差值就是这个特征给系统带来的信息量。所谓信息量，其实就是熵。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">信息量（熵）差值 —— 信息增益 —— 特征重要性</span><br></pre></td></tr></table></figure><font color="navy">其他特征选择方法本质上结合了某种或多种子集搜索和子集评价机制</font><p>而就方法论的角度来说，特征选择一共可以分为三大类：</p><ul><li>过滤</li><li>包裹</li><li>嵌入</li></ul><p>顾名思义也很好理解，过滤就是特征处理在机器学习之前，包裹是将机器学习模型本身的性能作为特征选择好坏的指标，换言之就是为某个模型量身定做的特征子集。</p><p>而嵌入式选择法就是将子集选择的过程与模型训练的过程结合在一起，譬如L1正则化。这里加入L1的损失函数的别名叫做LASSO回归，而加入L2的损失函数别名叫做岭回归。</p><p>关于L1, L2的讨论以及，何时取得最值的理解与证明，我觉得应该参考一下这个链接：<a href="https://www.zhihu.com/question/37096933" target="_blank" rel="noopener">L1为何比L2更易获得稀疏解</a>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">而所有的稀疏表达的本质就是为了降维</span><br></pre></td></tr></table></figure><p>过滤法里，最有名的方法就是Relief方法，Relief是针对二分类设计的算法，大概原理，就是遍历样本集，对每一个样本，计算猜对临近nearly hit, 和猜错临近nearly miss，通过计算相关属性的统计量，来衡量该属性对区分同类异类样本是否有益。</p><p>包裹法比较具有代表性的法则是LVW，就是las vegas wrapper，没什么好说的，N次遍历，每次随机选择特征子集，到达程序停止条件后输出子集。停止条件老生常谈的两种，要么循环次数达到上限，要么CrossValidation误差小于阈值。</p><p>嵌入法，就是上文的LASSO回归和岭回归</p><h4 id="稀疏学习"><a href="#稀疏学习" class="headerlink" title="稀疏学习"></a>稀疏学习</h4><p>周志华老师关于稀疏学习讲了两个例子，一个是字典学习，一个是压缩感知，我觉得这是从两个不同方向来对稀疏学习进行阐述 。稀疏学习首先要求训练样本有许多无效特征，这样的话就可以用稀疏矩阵表达，<font color="navy"><strong>核心还是为了降低学习难度、减少储存开销、增强学习模型的可解释性</strong></font>。</p><p>首先字典学习，感觉也没什么特殊的LASSO回归配合交替优化变量发与奇异值分解对损失函数的最小值进行求解。这里你需要知道<font color="navy"><strong>这种问题的解决方式的思路</strong></font>是怎样的。</p><p>然后再是压缩感知，压缩感知解决问题的思路如同泰勒展开式的脑洞一样神奇。它与特征选择和稀疏表示不同，压缩感知关注的是如何利用自身信号的稀疏性，从部分观测样本中恢复原信号，分为感知测量，和感知重构这两个阶段。</p><p>这个算法有一个比较好的介绍，贴在这里：<a href="https://zhuanlan.zhihu.com/p/22445302" target="_blank" rel="noopener">压缩感知算法介绍</a></p><h5 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h5><p>所以稀疏学习代表了一类问题的解决方式，就是特征矩阵过于稀疏，或是期望将特征矩阵变成稀疏的时候。下面我再结合经验谈谈稀疏学习，稀疏学习是近几年比较火热的一门技术，在信号处理（主要是压缩感知）、计算机视觉（比如JPEG压缩）领域影响比较大，而在机器学习领域里面，稀疏学习可以看做是一种<font color="navy"><strong>特征处理</strong></font>相关的模型。</p><p>那么说人话，<font color="navy">稀疏表示</font>是在超完备字典D（超完备是说字典的行数小于列数）中用尽可能少的原子来表示信号x，即：</p><p> $$min||α||_0\ \ \ \ \ s.t. \ \ x = Dα.$$  </p><p>考虑噪声，就是</p><p>$$min||x - Dα||_2^2+||α||_0$$</p><p>α的size要比x大很多，但是α的非零元素要比x少很多，也就是说α是个外强中干（非常稀疏）的向量，所以稀疏表示的核心，就是用若干个（尽可能少）的向量，去表示原向量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">稀疏即冗余</span><br></pre></td></tr></table></figure><p>当样本具有这样的稀疏表达形式的时候，对于学习任务有不少的好处，例如<font color="navy"><strong>支持向量机之所以能在文本数据上有很好的性能，恰好是由于文本数据使用了稀疏表示，使大多数问题变得线性可分</strong></font>。</p><p>而且稀疏表示在享受这么多好处的同时，并不会带来储存的负担。</p><p>稀疏模型研究方向主要包括系数求解（即上面那个问题，经典算法有OMP贪心、lasso凸松弛和L1/2非凸松弛），字典学习（获得更好的D，经典算法有MOD和K-SVD交替迭代）和模型应用。</p><p><font color="navy"><strong>显然稀疏表达的好坏与我们用的字典D有着密切的关系</strong></font>，字典分两类，一种是预先给定的分析字典，比如小波基、DCT等，另一种则是<font color="navy">针对特定数据集学习出特定的字典</font>。这种学出来的字典能大大提升在特定数据集的效果。</p><p>关于字典学习的模型，跟书里讲的差不多, 最小化系数表达与原表达的误差。</p><p>$$\max \limits_{D,W}||X-DW||_F^2\ \ \ \ s.t.||w_0|| \leq s$$</p><p>这个目标函数非凸，一般用交替迭代思想来解，即分别固定D和W，更新另一个。在Word2Vec的算法中，也体现了这种思想。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h4 id=&quot;特征选择&quot;&gt;&lt;a href=&quot;#特征选择
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>蒙特卡洛</title>
    <link href="http://yoursite.com/2018/06/22/20180622%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B/"/>
    <id>http://yoursite.com/2018/06/22/20180622蒙特卡洛/</id>
    <published>2018-06-22T05:39:13.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>蒙特卡罗算法并不是一种算法的名称，而是对一类随机算法的特性的概括。媒体说“蒙特卡罗算法打败武宫正树”，这个说法就好比说“我被一只脊椎动物咬了”，是比较火星的。</p><p>那么“蒙特卡罗”是一种什么特性呢？</p><p>我们知道，既然是随机算法，在采样不全时，通常不能保证找到最优解，只能说是尽量找。那么根据怎么个“尽量”法儿，我们我们把随机算法分成两类：</p><ul><li>蒙特卡罗算法：采样越多，越近似最优解；</li><li>拉斯维加斯算法：采样越多，越有机会找到最优解；</li></ul><p>举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找好的，但不保证是最好的。而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（最优解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。</p><p>所以蒙特卡洛本质上能够应对遍历无放回的最优值情况，而拉斯维加斯适合求解独立同分布的情况。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;蒙特卡罗算法并不是一种算法的名称，而是对一类随机算法的特性的概括。媒体说“蒙特卡罗算法打败武宫正树”，这个说法就好比说“我被一只脊椎动物咬了”，是比较火星的。&lt;/p&gt;
&lt;p&gt;那么“蒙特卡罗”是一种什么特性呢？&lt;/p&gt;
&lt;p&gt;我们知道，既然是随机算法，在采样不全时，通常不能保
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>潜意识</title>
    <link href="http://yoursite.com/2018/06/21/20180621%E6%BD%9C%E6%84%8F%E8%AF%86/"/>
    <id>http://yoursite.com/2018/06/21/20180621潜意识/</id>
    <published>2018-06-21T02:15:58.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>始终有两个我，在做斗争，可能是左右脑分工的区别。有一句话说的非常好：</p><font color="navy"><strong>除非你意识到你的潜意识，否则潜意识将主导你的人生，而你将其称为命运。</strong></font><p>我们的应激反应，陋习与缺点，都是我们感性大脑急于获得快感而发出的指令。包括高热量的食品，以及无节制的性爱，都算是人类对及时行乐的一个基因表达。<br>人的独特之处不仅仅在于他有语言和逻辑，而在于他能跳出来审视自己的思维的对错，如果我们时时刻刻能够提醒自己，不要让我们的潜意识主导我们的行为，那我们能够成长为一个非常值得尊敬的人。管理自己身材，头脑，情绪都算这个范围内。</p><p>当然我也清楚，永远做应该做的事情是非常勇敢的，可能要承担很多，但是No pains no gains，过去我的选择，或多或少都已经奖励或者惩罚我了。而在奔三的道路上，希望自己能够重拾当年的勇气，达到一个前所未有的高度，辨材须待七年期，君自珍重。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;始终有两个我，在做斗争，可能是左右脑分工的区别。有一句话说的非常好：&lt;/p&gt;
&lt;font color=&quot;navy&quot;&gt;&lt;strong&gt;除非你意识到你的潜意识，否则潜意识将主导你的人生，而你将其称为命运。&lt;/strong&gt;&lt;/font&gt;



&lt;p&gt;我们的应激反应，陋习与缺点，
      
    
    </summary>
    
      <category term="Muse" scheme="http://yoursite.com/categories/Muse/"/>
    
    
      <category term="Better Me" scheme="http://yoursite.com/tags/Better-Me/"/>
    
  </entry>
  
  <entry>
    <title>Self-Discipline</title>
    <link href="http://yoursite.com/2018/06/18/20180618Self-discipline/"/>
    <id>http://yoursite.com/2018/06/18/20180618Self-discipline/</id>
    <published>2018-06-18T11:18:19.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>张公子说：所有炫目的才华，都来自于过溢的基本功</p><p>于我心有戚戚焉，老话说台上一分钟，台下十年功，谈笑风生是需要肚子里有货的。而这十年功怎么积累，是个关键。<br>我们大部分人有自己的梦想，但是为自己的梦想做出牺牲的人太少太少。人类的进化决定了人总是倾向于让自己朝着舒适的方向去发展，而逃避那些让自己痛苦的事情，每个人最大的资本就是一天的24小时，如何去安排时间，如何自律，将在很大程度上决定你的个人高度。虽然以前我也明白这一点，但是长久以来人会麻木。就像我从《原则》感触到的，节省时间的一个好办法就是将自己要做的事情养成习惯，这就好比宏编程一样，可以很大程度上减少大脑的犹豫和纠结。</p><p>C罗就是一个极度自律的人，照理说功成名就后，可是在Ins上看到他po的状态，还是打心眼儿里佩服这样一个巨星。</p><font color="navy">C罗在曼联效力时，每天至少花一个小时锻炼腰腹和肌肉，自从转会皇马后，更加的疯狂，每天2000个仰卧起坐。这已成了C罗的人生日常，自律成了一种习惯。</font><p>越是站在世界之巅，越懂得自律的意义！</p><p><strong>Nothing worth comes easily</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;张公子说：所有炫目的才华，都来自于过溢的基本功&lt;/p&gt;
&lt;p&gt;于我心有戚戚焉，老话说台上一分钟，台下十年功，谈笑风生是需要肚子里有货的。而这十年功怎么积累，是个关键。&lt;br&gt;我们大部分人有自己的梦想，但是为自己的梦想做出牺牲的人太少太少。人类的进化决定了人总是倾向于让自己朝
      
    
    </summary>
    
    
      <category term="Better Me" scheme="http://yoursite.com/tags/Better-Me/"/>
    
  </entry>
  
  <entry>
    <title>TextRank摘要算法实现原理</title>
    <link href="http://yoursite.com/2018/05/31/20180531TextRank%E6%91%98%E8%A6%81%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2018/05/31/20180531TextRank摘要算法实现原理/</id>
    <published>2018-05-31T11:09:49.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>所谓自动摘要，就是从文章中自动抽取关键句。何谓关键句？人类的理解是能够概括文章中心的句子，机器的理解只能模拟人类的理解，即拟定一个权重的评分标准，给每个句子打分，之后给出排名靠前的几个句子。</p><h4 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h4><p>TextRank的打分思想依然是从PageRank的迭代思想衍生过来的，如下公式所示：</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180531TR-02.jpg" alt="创新扩散曲线"></p><p>等式左边表示一个句子的权重（WS是weight_sum的缩写），右侧的求和表示每个相邻句子对本句子的贡献程度。与提取关键字的时候不同，一般认为<font color="red"><strong>全部句子都是相邻的</strong></font>，不再提取窗口。</p><p>求和的分子wji表示两个句子的相似程度，分母又是一个weight_sum，而WS(Vj)代表上次迭代j的权重。整个公式是一个迭代的过程。</p><h4 id="相似度计算-1"><a href="#相似度计算-1" class="headerlink" title="相似度计算"></a>相似度计算</h4><p>而相似程度Wij的计算，推荐使用BM25</p><p>BM25算法，通常用来作搜索相关性评分。一句话概况其主要思想：对Query进行语素解析，生成语素qi；然后，对于每个搜索结果D，计算每个语素qi与D的相关性得分，最后，将qi相对于D的相关性得分进行加权求和，从而得到Query与D的相关性得分。</p><p><a href="https://www.jianshu.com/p/1e498888f505" target="_blank" rel="noopener">算法连接</a></p><h4 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">算法可大致分为基本算法、数据结构的算法、数论算法、计算几何的算法、图的算法、动态规划以及数值分析、加密算法、排序算法、检索算法、随机化算法、并行算法、厄米变形模型、随机森林算法。</span><br><span class="line"></span><br><span class="line">算法可以宽泛的分为三类，</span><br><span class="line"></span><br><span class="line">一，有限的确定性算法，这类算法在有限的一段时间内终止。他们可能要花很长时间来执行指定的任务，但仍将在一定的时间内终止。这类算法得出的结果常取决于输入值。</span><br><span class="line"></span><br><span class="line">二，有限的非确定算法，这类算法在有限的时间内终止。然而，对于一个（或一些）给定的数值，算法的结果并不是唯一的或确定的。</span><br><span class="line"></span><br><span class="line">三，无限的算法，是那些由于没有定义终止定义条件，或定义的条件无法由输入的数据满足而不终止运行的算法。通常，无限算法的产生是由于未能确定的定义终止条件。</span><br></pre></td></tr></table></figure><h4 id="断句"><a href="#断句" class="headerlink" title="断句"></a>断句</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">算法可大致分为基本算法、数据结构的算法、数论算法、计算几何的算法、图的算法、动态规划以及数值分析、加密算法、排序算法、检索算法、随机化算法、并行算法、厄米变形模型、随机森林算法</span><br><span class="line">算法可以宽泛的分为三类</span><br><span class="line">一</span><br><span class="line">有限的确定性算法</span><br><span class="line">这类算法在有限的一段时间内终止</span><br><span class="line">他们可能要花很长时间来执行指定的任务</span><br><span class="line">但仍将在一定的时间内终止</span><br><span class="line">这类算法得出的结果常取决于输入值</span><br><span class="line">二</span><br><span class="line">有限的非确定算法</span><br><span class="line">这类算法在有限的时间内终止</span><br><span class="line">然而</span><br><span class="line">对于一个（或一些）给定的数值</span><br><span class="line">算法的结果并不是唯一的或确定的</span><br><span class="line">三</span><br><span class="line">无限的算法</span><br><span class="line">是那些由于没有定义终止定义条件</span><br><span class="line">或定义的条件无法由输入的数据满足而不终止运行的算法</span><br><span class="line">通常</span><br><span class="line">无限算法的产生是由于未能确定的定义终止条件</span><br></pre></td></tr></table></figure><h4 id="分词并过滤停用词"><a href="#分词并过滤停用词" class="headerlink" title="分词并过滤停用词"></a>分词并过滤停用词</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[算法, 大致, 分, 基本, 算法, 数据, 结构, 算法, 数论, 算法, 计算, 几何, 算法, 图, 算法, 动态, 规划, 数值, 分析, 加密, 算法, 排序, 算法, 检索, 算法, 随机, 化, 算法, 并行, 算法, 厄, 米, 变形, 模型, 随机, 森林, 算法]</span><br><span class="line">[算法, 宽泛, 分为, 三类]</span><br><span class="line">[]</span><br><span class="line">[有限, 确定性, 算法]</span><br><span class="line">[类, 算法, 有限, 一段, 时间, 终止]</span><br><span class="line">[可能, 花, 长, 时间, 执行, 指定, 任务]</span><br><span class="line">[一定, 时间, 终止]</span><br><span class="line">[类, 算法, 得出, 常, 取决, 输入, 值]</span><br><span class="line">[二]</span><br><span class="line">[有限, 非, 确定, 算法]</span><br><span class="line">[类, 算法, 有限, 时间, 终止]</span><br><span class="line">[]</span><br><span class="line">[一个, 定, 数值]</span><br><span class="line">[算法, 唯一, 确定]</span><br><span class="line">[三]</span><br><span class="line">[无限, 算法]</span><br><span class="line">[没有, 定义, 终止, 定义, 条件]</span><br><span class="line">[定义, 条件, 无法, 输入, 数据, 满足, 终止, 运行, 算法]</span><br><span class="line">[通常]</span><br><span class="line">[无限, 算法, 产生, 未, 确定, 定义, 终止, 条件]</span><br></pre></td></tr></table></figure><h4 id="计算BM25相关矩阵"><a href="#计算BM25相关矩阵" class="headerlink" title="计算BM25相关矩阵"></a>计算BM25相关矩阵</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[15.176530737482341, -2.604484103028904, 0.0, -2.8740684265166565, -2.1930693258940175, 0.0, 0.0, -2.0325355810136103, 0.0, -2.604484103028904, -2.3811362523642052, 0.0, 2.509043358515279, -2.8740684265166565, 0.0, -3.2059044218809922, 0.0, -0.22517864251663589, 0.0, -1.8939010965185548]</span><br><span class="line">[-0.2864022115473306, 8.52437122545896, 0.0, -0.23950570220972142, -0.18275577715783484, 0.0, 0.0, -0.1693779650844675, 0.0, -0.21704034191907534, -0.19842802103035043, 0.0, 0.0, -0.23950570220972142, 0.0, -0.267158701823416, 0.0, -0.1477475757866994, 0.0, -0.15782509137654627]</span><br><span class="line">[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</span><br><span class="line">[-0.2864022115473306, -0.21704034191907534, 0.0, 4.604672851367114, 1.060086217315166, 0.0, 0.0, -0.1693779650844675, 0.0, 1.2589559610532894, 1.1509940396671094, 0.0, 0.0, -0.23950570220972142, 0.0, -0.267158701823416, 0.0, -0.1477475757866994, 0.0, -0.15782509137654627]</span><br><span class="line">[-0.2864022115473306, -0.21704034191907534, 0.0, 1.3892676764009562, 7.063472116341414, 1.1518653539666401, 2.634590118176154, 1.2574519044179069, 0.0, 1.2589559610532894, 5.005270773642655, 0.0, 0.0, -0.23950570220972142, 0.0, -0.267158701823416, 0.8333088661764476, 0.4727261064071153, 0.0, 0.504969645305668]</span><br><span class="line">[0.0, 0.0, 0.0, 0.0, 1.2428419944730007, 14.795434933306574, 1.6287733786106775, 0.0, 0.0, 0.0, 1.3494220606974598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</span><br><span class="line">[0.0, 0.0, 0.0, 0.0, 2.010334451736872, 1.1518653539666401, 5.849995293142312, 0.0, 0.0, 0.0, 2.1827309268739077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8333088661764476, 0.6204736821938147, 0.0, 0.6627947366822143]</span><br><span class="line">[-0.2864022115473306, -0.21704034191907534, 0.0, -0.23950570220972142, 1.356767982871274, 0.0, 0.0, 12.127555522767913, 0.0, -0.21704034191907534, 1.4731177860712878, 0.0, 0.0, -0.23950570220972142, 0.0, -0.267158701823416, 0.0, 1.4000446911370572, 0.0, -0.15782509137654627]</span><br><span class="line">[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.054814792796337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</span><br><span class="line">[-0.2864022115473306, -0.21704034191907534, 0.0, 1.3892676764009562, 1.060086217315166, 0.0, 0.0, -0.1693779650844675, 0.0, 6.001094704342757, 1.1509940396671094, 0.0, 0.0, 1.7780760396570634, 0.0, -0.267158701823416, 0.0, -0.1477475757866994, 0.0, 1.1716840594784514]</span><br><span class="line">[-0.2864022115473306, -0.21704034191907534, 0.0, 1.3892676764009562, 4.609944429081147, 1.1518653539666401, 2.634590118176154, 1.2574519044179069, 0.0, 1.2589559610532894, 5.005270773642655, 0.0, 0.0, -0.23950570220972142, 0.0, -0.267158701823416, 0.8333088661764476, 0.4727261064071153, 0.0, 0.504969645305668]</span><br><span class="line">[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</span><br><span class="line">[0.5551884973225691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.939853708447595, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</span><br><span class="line">[-0.2864022115473306, -0.21704034191907534, 0.0, -0.23950570220972142, -0.18275577715783484, 0.0, 0.0, -0.1693779650844675, 0.0, 1.611294545577714, -0.19842802103035043, 0.0, 0.0, 4.9934812146232215, 0.0, -0.267158701823416, 0.0, -0.1477475757866994, 0.0, 1.1716840594784514]</span><br><span class="line">[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.054814792796337, 0.0, 0.0, 0.0, 0.0, 0.0]</span><br><span class="line">[-0.2864022115473306, -0.21704034191907534, 0.0, -0.23950570220972142, -0.18275577715783484, 0.0, 0.0, -0.1693779650844675, 0.0, -0.21704034191907534, -0.19842802103035043, 0.0, 0.0, -0.23950570220972142, 0.0, 2.531575358765468, 0.0, -0.1477475757866994, 0.0, 1.4955384555950606]</span><br><span class="line">[0.0, 0.0, 0.0, 0.0, 0.7674924572638717, 0.0, 1.0058167395654765, 0.0, 0.0, 0.0, 0.8333088661764476, 0.0, 0.0, 0.0, 0.0, 0.0, 9.892547495751218, 4.354323965031352, 0.0, 4.651322189247207]</span><br><span class="line">[0.26878628577523855, -0.21704034191907534, 0.0, -0.23950570220972142, 0.5847366801060369, 0.0, 1.0058167395654765, 1.6050126003722507, 0.0, -0.21704034191907534, 0.6348808451460972, 0.0, 0.0, -0.23950570220972142, 0.0, -0.267158701823416, 4.866735958438866, 12.008153881124132, 0.0, 3.1639879470156633]</span><br><span class="line">[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.054814792796337, 0.0]</span><br><span class="line">[-0.2864022115473306, -0.21704034191907534, 0.0, -0.23950570220972142, 0.5847366801060369, 0.0, 1.0058167395654765, -0.1693779650844675, 0.0, 1.611294545577714, 0.6348808451460972, 0.0, 0.0, 1.7780760396570634, 0.0, 2.531575358765468, 4.866735958438866, 2.9619596282988065, 0.0, 10.38451854500608]</span><br></pre></td></tr></table></figure><h4 id="迭代投票"><a href="#迭代投票" class="headerlink" title="迭代投票"></a>迭代投票</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">for (int _ = 0; _ &lt; max_iter; ++_)</span><br><span class="line">&#123;</span><br><span class="line">    double[] m = new double[D];</span><br><span class="line">    double max_diff = 0;</span><br><span class="line">    for (int i = 0; i &lt; D; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        m[i] = 1 - d;</span><br><span class="line">        for (int j = 0; j &lt; D; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            if (j == i || weight_sum[j] == 0) continue;</span><br><span class="line">            m[i] += (d * weight[j][i] / weight_sum[j] * vertex[j]);</span><br><span class="line">        &#125;</span><br><span class="line">        double diff = Math.abs(m[i] - vertex[i]);</span><br><span class="line">        if (diff &gt; max_diff)</span><br><span class="line">        &#123;</span><br><span class="line">            max_diff = diff;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    vertex = m;</span><br><span class="line">    if (max_diff &lt;= min_diff) break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="输出排序结果"><a href="#输出排序结果" class="headerlink" title="输出排序结果"></a>输出排序结果</h4><ul><li>这类算法在有限的时间内终止</li><li>这类算法在有限的一段时间内终止</li><li>无限算法的产生是由于未能确定的定义终止条件</li></ul><p>效果还可以。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;所谓自动摘要，就是从文章中自动抽取关键句。何谓关键句？人类的理解是能够概括文章中心的句子，机器的理解只能模拟人类的理解，即拟定一个权重的评分标准，给每个句子打分，之后给出排名靠前的几个句子。&lt;/p&gt;
&lt;h4 id=&quot;相似度计算&quot;&gt;&lt;a href=&quot;#相似度计算&quot; class
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>TextRank提取关键词实现原理</title>
    <link href="http://yoursite.com/2018/05/31/20180531TextRank%E6%8F%90%E5%8F%96%E5%85%B3%E9%94%AE%E8%AF%8D%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2018/05/31/20180531TextRank提取关键词实现原理/</id>
    <published>2018-05-31T06:51:12.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>TextRank是在Google的PageRank算法启发下，针对文本里的句子设计的权重算法，目标是自动摘要。它利用投票的原理，让每一个单词给它的邻居（术语称窗口）投赞成票，票的权重取决于自己的票数。这是一个“先有鸡还是先有蛋”的悖论，PageRank采用矩阵迭代收敛的方式解决了这个悖论。TextRank也不例外：</p><h4 id="PageRank的计算公式："><a href="#PageRank的计算公式：" class="headerlink" title="PageRank的计算公式："></a>PageRank的计算公式：</h4><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180531TR-01.jpg" alt="创新扩散曲线"></p><h4 id="TextRank的计算公式："><a href="#TextRank的计算公式：" class="headerlink" title="TextRank的计算公式："></a>TextRank的计算公式：</h4><p>正规的TextRank公式在PageRank的公式的基础上，引入了边的权值的概念，<font color="red"><strong>代表两个句子的相似度</strong></font>。<br><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180531TR-02.jpg" alt="创新扩散曲线"></p><p>但是很明显我只想计算关键字，如果把一个单词视为一个句子的话，那么所有句子（单词）构成的边的权重都是0（没有交集，没有相似性），所以分子分母的权值w约掉了，算法退化为PageRank。所以说，这里称关键字提取算法为PageRank也不为过。</p><h4 id="Java实现"><a href="#Java实现" class="headerlink" title="Java实现"></a>Java实现</h4><p>先看看测试数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">程序员(英文Programmer)是从事程序开发、维护的专业人员。</span><br><span class="line">一般将程序员分为程序设计人员和程序编码人员，但两者的</span><br><span class="line">界限并不非常清楚，特别是在中国。软件从业人员分为初级</span><br><span class="line">程序员、高级程序员、系统分析员和项目经理四大类。</span><br></pre></td></tr></table></figure><p>我取出了百度百科关于“程序员”的定义作为测试用例，很明显，这段定义的关键字应当是“程序员”并且“程序员”的得分应当最高。</p><p>首先对这句话分词，这里可以借助各种分词项目，比如HanLP分词，得出分词结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[程序员/n, (, 英文/nz, programmer/en, ), 是/v, 从事/v, 程序/n, </span><br><span class="line">开发/v, 、/w, 维护/v, 的/uj, 专业/n, 人员/n, 。/w, 一般/a, </span><br><span class="line">将/d, 程序员/n, 分为/v, 程序/n, 设计/vn, 人员/n, 和/c, </span><br><span class="line">程序/n, 编码/n, 人员/n, ，/w, 但/c, 两者/r, 的/uj, 界限/n, </span><br><span class="line">并/c, 不/d, 非常/d, 清楚/a, ，/w, 特别/d, 是/v, 在/p, 中国/ns, 。</span><br><span class="line">/w, 软件/n, 从业/b, 人员/n, 分为/v, 初级/b, 程序员/n, 、</span><br><span class="line">/w, 高级/a, 程序员/n, 、/w, 系统/n, 分析员/n, 和/c, 项目/n,</span><br><span class="line"> 经理/n, 四/m, 大/a, 类/q, 。/w]</span><br></pre></td></tr></table></figure><p>然后去掉里面的停用词，这里我去掉了标点符号、常用词、以及“名词、动词、形容词、副词之外的词”。得出实际有用的词语：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[程序员, 英文, 程序, 开发, 维护, 专业, 人员, 程序员, </span><br><span class="line">分为, 程序, 设计, 人员, 程序, 编码, 人员, 界限, 特别, </span><br><span class="line">中国, 软件, 人员, 分为, 程序员, 高级, 程序员, 系统, </span><br><span class="line">分析员, 项目, 经理]</span><br></pre></td></tr></table></figure><p>下面是代码实现的关键：</p><font color="navy">之后建立两个大小为5的窗口，每个单词将票投给它身前身后距离5以内的单词（中括号的词排名不分先后，但原始分词顺序不可以打乱）：</font><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#123;开发=[专业, 程序员, 维护, 英文, 程序, 人员],</span><br><span class="line"></span><br><span class="line"> 软件=[程序员, 分为, 界限, 高级, 中国, 特别, 人员],</span><br><span class="line"></span><br><span class="line"> 程序员=[开发, 软件, 分析员, 维护, 系统, 项目, 经理, 分为, 英文, 程序, 专业, 设计, 高级, 人员, 中国],</span><br><span class="line"></span><br><span class="line"> 分析员=[程序员, 系统, 项目, 经理, 高级],</span><br><span class="line"></span><br><span class="line"> 维护=[专业, 开发, 程序员, 分为, 英文, 程序, 人员],</span><br><span class="line"></span><br><span class="line"> 系统=[程序员, 分析员, 项目, 经理, 分为, 高级],</span><br><span class="line"></span><br><span class="line"> 项目=[程序员, 分析员, 系统, 经理, 高级],</span><br><span class="line"></span><br><span class="line"> 经理=[程序员, 分析员, 系统, 项目],</span><br><span class="line"></span><br><span class="line"> 分为=[专业, 软件, 设计, 程序员, 维护, 系统, 高级, 程序, 中国, 特别, 人员],</span><br><span class="line"></span><br><span class="line"> 英文=[专业, 开发, 程序员, 维护, 程序],</span><br><span class="line"></span><br><span class="line"> 程序=[专业, 开发, 设计, 程序员, 编码, 维护, 界限, 分为, 英文, 特别, 人员],</span><br><span class="line"></span><br><span class="line"> 特别=[软件, 编码, 分为, 界限, 程序, 中国, 人员],</span><br><span class="line"></span><br><span class="line"> 专业=[开发, 程序员, 维护, 分为, 英文, 程序, 人员],</span><br><span class="line"></span><br><span class="line"> 设计=[程序员, 编码, 分为, 程序, 人员],</span><br><span class="line"></span><br><span class="line"> 编码=[设计, 界限, 程序, 中国, 特别, 人员],</span><br><span class="line"></span><br><span class="line"> 界限=[软件, 编码, 程序, 中国, 特别, 人员],</span><br><span class="line"></span><br><span class="line"> 高级=[程序员, 软件, 分析员, 系统, 项目, 分为, 人员],</span><br><span class="line"></span><br><span class="line"> 中国=[程序员, 软件, 编码, 分为, 界限, 特别, 人员],</span><br><span class="line"></span><br><span class="line"> 人员=[开发, 程序员, 软件, 维护, 分为, 程序, 特别, 专业, 设计, 编码, 界限, 高级, 中国]&#125;</span><br></pre></td></tr></table></figure><p>然后开始迭代投票，代码没什么难的，就是按照原论文算法过程简单的实现了一遍，这里简单给个注释，省得以后看起来麻烦：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">for (int i = 0; i &lt; max_iter; ++i) //最外层条件是算法设定的最大迭代次数</span><br><span class="line">       &#123;</span><br><span class="line">           Map&lt;String, Float&gt; m = new HashMap&lt;String, Float&gt;(); //&lt;单词，分数&gt;</span><br><span class="line">           float max_diff = 0; //算法终止收敛值</span><br><span class="line">           //一个entry代表一个窗口列表，例如： 设计=[程序员, 编码, 分为, 程序, 人员]</span><br><span class="line">           //按照这个entry来举例解释下面的代码</span><br><span class="line">           for (Map.Entry&lt;String, Set&lt;String&gt;&gt; entry : words.entrySet()) </span><br><span class="line">           &#123;</span><br><span class="line">               String key = entry.getKey();  //设计</span><br><span class="line">               Set&lt;String&gt; value = entry.getValue();  //[程序员, 编码, 分为, 程序, 人员]</span><br><span class="line">               m.put(key, 1 - d);  //公式里面的（1-d）</span><br><span class="line">               for (String other : value) //对value列表中的单词进行遍历</span><br><span class="line">               &#123;</span><br><span class="line">                   int size = words.get(other).size(); //单词的度</span><br><span class="line">                   if (key.equals(other) || size == 0) continue; //保证列表单词与待求单词不同</span><br><span class="line">                   m.put(key, m.get(key) + d / size * (score.get(other) == null ? 0 : score.get(other)));</span><br><span class="line">               &#125;</span><br><span class="line">               //每次计算分数后要计算误差与收敛值的差值</span><br><span class="line">               max_diff = Math.max(max_diff, Math.abs(m.get(key) - (score.get(key) == null ? 0 : score.get(key))));</span><br><span class="line">           &#125;</span><br><span class="line">           score = m;</span><br><span class="line">           if (max_diff &lt;= min_diff) break;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><p>排序后的投票结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[程序员=1.9249977,</span><br><span class="line"></span><br><span class="line">人员=1.6290349,</span><br><span class="line"></span><br><span class="line">分为=1.4027836,</span><br><span class="line"></span><br><span class="line">程序=1.4025855,</span><br><span class="line"></span><br><span class="line">高级=0.9747374,</span><br><span class="line"></span><br><span class="line">软件=0.93525416,</span><br><span class="line"></span><br><span class="line">中国=0.93414587,</span><br><span class="line"></span><br><span class="line">特别=0.93352026,</span><br><span class="line"></span><br><span class="line">维护=0.9321688,</span><br><span class="line"></span><br><span class="line">专业=0.9321688,</span><br><span class="line"></span><br><span class="line">系统=0.885048,</span><br><span class="line"></span><br><span class="line">编码=0.82671607,</span><br><span class="line"></span><br><span class="line">界限=0.82206935,</span><br><span class="line"></span><br><span class="line">开发=0.82074183,</span><br><span class="line"></span><br><span class="line">分析员=0.77101076,</span><br><span class="line"></span><br><span class="line">项目=0.77101076,</span><br><span class="line"></span><br><span class="line">英文=0.7098714,</span><br><span class="line"></span><br><span class="line">设计=0.6992446,</span><br><span class="line"></span><br><span class="line">经理=0.64640945]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;TextRank是在Google的PageRank算法启发下，针对文本里的句子设计的权重算法，目标是自动摘要。它利用投票的原理，让每一个单词给它的邻居（术语称窗口）投赞成票，票的权重取决于自己的票数。这是一个“先有鸡还是先有蛋”的悖论，PageRank采用矩阵迭代收敛的方式
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vector原始版本过程深度解析</title>
    <link href="http://yoursite.com/2018/05/29/20180529Word2Vector%E8%BF%87%E7%A8%8B%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2018/05/29/20180529Word2Vector过程深度解析/</id>
    <published>2018-05-29T08:50:30.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>此文主要讲述未用哈夫曼二叉树优化的w2v计算过程，可以对其神经网络对自然语言处理的过程有个基本的认识。</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180529-W2V-01.jpg" alt="创新扩散曲线"></p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180529-W2V-02.jpg" alt="创新扩散曲线"></p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180529-W2V-03.jpg" alt="创新扩散曲线"></p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180529-W2V-04.jpg" alt="创新扩散曲线"></p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180529-W2V-05.jpg" alt="创新扩散曲线"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;此文主要讲述未用哈夫曼二叉树优化的w2v计算过程，可以对其神经网络对自然语言处理的过程有个基本的认识。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/pictur
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>NLP之中文分词实现原理</title>
    <link href="http://yoursite.com/2018/05/28/20180528NLP%E4%B9%8B%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2018/05/28/20180528NLP之中文分词实现原理/</id>
    <published>2018-05-28T07:04:06.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>目前中文分词三大主流分词方法：基于词典的方法、基于规则的方法和基于统计的方法</p><h3 id="基于词典的方法"><a href="#基于词典的方法" class="headerlink" title="基于词典的方法"></a>基于词典的方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">定义：按照一定策略将待分析的汉字串与一个“大机器词典”中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。</span><br></pre></td></tr></table></figure><ul><li>按照扫描方向的不同：正向匹配和逆向匹配</li><li>按照长度的不同：最大匹配和最小匹配</li></ul><h4 id="正向最大匹配思想MM"><a href="#正向最大匹配思想MM" class="headerlink" title="正向最大匹配思想MM"></a>正向最大匹配思想MM</h4><ul><li>从左向右取待切分汉语句的m个字符作为匹配字段，m为大机器词典中最长词条个数。</li><li>查找大机器词典并进行匹配： <ul><li>若匹配成功，则将这个匹配字段作为一个词切分出来。</li><li>若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配，重复以上过程，直到切分出所有词为止。</li></ul></li></ul><p>举个栗子： </p><p>现在，我们要对“南京市长江大桥”这个句子进行分词，根据正向最大匹配的原则：</p><ul><li>先从句子中拿出前5个字符“南京市长江”，把这5个字符到词典中匹配，发现没有这个词，那就缩短取字个数，取前四个“南京市长”，发现词库有这个词，就把该词切下来；</li><li>对剩余三个字“江大桥”再次进行正向最大匹配，会切成“江”、“大桥”；</li><li>整个句子切分完成为：南京市长、江、大桥；</li></ul><h4 id="逆向最大匹配算法RMM"><a href="#逆向最大匹配算法RMM" class="headerlink" title="逆向最大匹配算法RMM"></a>逆向最大匹配算法RMM</h4><p>该算法是正向最大匹配的逆向思维，匹配不成功，将匹配字段的最前一个字去掉，实验表明，逆向最大匹配算法要优于正向最大匹配算法。</p><p>还是那个栗子：</p><ul><li>取出“南京市长江大桥”的后四个字“长江大桥”，发现词典中有匹配，切割下来；</li><li>对剩余的“南京市”进行分词，整体结果为：南京市、长江大桥</li></ul><h4 id="双向最大匹配法-Bi-directction-Matching-method-BM"><a href="#双向最大匹配法-Bi-directction-Matching-method-BM" class="headerlink" title="双向最大匹配法(Bi-directction Matching method,BM)"></a>双向最大匹配法(Bi-directction Matching method,BM)</h4><p>双向最大匹配法是将正向最大匹配法得到的分词结果和逆向最大匹配法的到的结果进行比较，从而决定正确的分词方法。</p><p>据SunM.S. 和 Benjamin K.T.（1995）的研究表明，中文中90.0％左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概9.0％的句子两种切分方法得到的结果不一样，但其中必有一个是正确的（歧义检测成功），只有不到1.0％的句子，或者正向最大匹配法和逆向最大匹配法的切分虽重合却是错的，或者正向最大匹配法和逆向最大匹配法切分不同但两个都不对（歧义检测失败）。这正是双向最大匹配法在实用中文信息处理系统中得以广泛使用的原因所在。</p><p>还是那个栗子： </p><p>双向的最大匹配，即把所有可能的最大词都分出来，上面的句子可以分为：南京市、南京市长、长江大桥、江、大桥</p><h4 id="设立切分标志法"><a href="#设立切分标志法" class="headerlink" title="设立切分标志法"></a>设立切分标志法</h4><p>收集切分标志，在自动分词前处理切分标志，再用MM、RMM进行细加工。</p><h4 id="最佳匹配（OM，分正向和逆向）"><a href="#最佳匹配（OM，分正向和逆向）" class="headerlink" title="最佳匹配（OM，分正向和逆向）"></a>最佳匹配（OM，分正向和逆向）</h4><p>对分词词典按词频大小顺序排列，并注明长度，降低时间复杂度。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">优点：易于实现 </span><br><span class="line">缺点：匹配速度慢。对于未登录词的补充较难实现。缺乏自学习。</span><br></pre></td></tr></table></figure><h4 id="逐词遍历法"><a href="#逐词遍历法" class="headerlink" title="逐词遍历法"></a>逐词遍历法</h4><p>这种方法是将词库中的词由长到短递减的顺序，逐个在待处理的材料中搜索，直到切分出所有的词为止。 </p><p>处理以上基本的机械分词方法外，还有双向扫描法、二次扫描法、基于词频统计的分词方法、联想—回溯法等。</p><h3 id="基于统计的分词"><a href="#基于统计的分词" class="headerlink" title="基于统计的分词"></a>基于统计的分词</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">随着大规模语料库的建立，统计机器学习方法的研究和发展，基于统计的中文分词方法渐渐成为了主流方法。</span><br></pre></td></tr></table></figure><p><strong>主要思想</strong>：把每个词看做是由词的最小单位各个字总成的，如果相连的字在不同的文本中出现的次数越多，就证明这相连的字很可能就是一个词。因此我们就可以利用字与字相邻出现的频率来反应成词的可靠度，统计语料中相邻共现的各个字的组合的频度，当组合频度高于某一个临界值时，我们便可认为此字组可能会构成一个词语。</p><p><strong>主要统计模型</strong>：N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model ，HMM），最大熵模型（ME），条件随机场模型（Conditional Random Fields，CRF）等。</p><p><strong>优势</strong>：在实际的应用中经常是将分词词典串匹配分词和统计分词能较好地识别新词两者结合起来使用，这样既体现了匹配分词切分不仅速度快，而且效率高的特点；同时又能充分地利用统计分词在结合上下文识别生词、自动消除歧义方面的优点。</p><h4 id="N-gram模型思想"><a href="#N-gram模型思想" class="headerlink" title="N-gram模型思想"></a>N-gram模型思想</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">模型基于这样一种假设，第n个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。</span><br></pre></td></tr></table></figure><p>我们给定一个词，然后猜测下一个词是什么。当我说“艳照门”这个词时，你想到下一个词是什么呢？我想大家很有可能会想到“陈冠希”，基本上不会有人会想到“陈志杰”吧，N-gram模型的主要思想就是这样的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对于一个句子T，我们怎么算它出现的概率呢？假设T是由词序列W1,W2,W3,…Wn组成的，那么P(T)=P(W1W2W3…Wn)=P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1) </span><br><span class="line"></span><br><span class="line">但是这种方法存在两个致命的缺陷：一个缺陷是参数空间过大，不可能实用化；另外一个缺陷是数据稀疏严重。为了解决这个问题，我们引入了马尔科夫假设：一个词的出现仅仅依赖于它前面出现的有限的一个或者几个词。如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram。即</span><br><span class="line"></span><br><span class="line">P(T) =P(W1W2W3…Wn)=P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1) </span><br><span class="line">≈P(W1)P(W2|W1)P(W3|W2)…P(Wn|Wn-1)</span><br></pre></td></tr></table></figure><p>如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram。</p><p>在实践中用的最多的就是bigram和trigram了，而且效果很不错。高于四元的用的很少，因为训练它需要更庞大的语料，而且数据稀疏严重，时间复杂度高，精度却提高的不多。一般的小公司，用到二元的模型就够了，像Google这种巨头，也只是用到了大约四元的程度，它对计算能力和空间的需求都太大了。</p><p>以此类推，N元模型就是假设当前词的出现概率只同它前面的N-1个词有关。</p><h4 id="HMM、CRF-模型思想"><a href="#HMM、CRF-模型思想" class="headerlink" title="HMM、CRF 模型思想"></a>HMM、CRF 模型思想</h4><p>以往的分词方法，无论是基于规则的还是基于统计的，一般都依赖于一个事先编制的词表(词典)，自动分词过程就是通过词表和相关信息来做出词语切分的决策。与此相反，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基于字标注（或者叫基于序列标注）的分词方法实际上是构词方法，即把分词过程视为字在字串中的标注问题。</span><br></pre></td></tr></table></figure><p>由于每个字在构造一个特定的词语时都占据着一个确定的构词位置(即词位)，假如规定每个字最多只有四个构词位置：即B(词首)，M (词中)，E(词尾)和S(单独成词)，那么下面句子(甲)的分词结果就可以直接表示成如(乙)所示的逐字标注形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(甲)分词结果：／上海／计划／N／本／世纪／末／实现／人均／国内／生产／总值／五千美元／ </span><br><span class="line">(乙)字标注形式：上／B海／E计／B划／E N／S 本／s世／B 纪／E 末／S 实／B 现／E 人／B 均／E 国／B 内／E生／B产／E总／B值／E 五／B千／M 美／M 元／E 。／S</span><br></pre></td></tr></table></figure><p>首先需要说明，这里说到的“字”不只限于汉字。考虑到中文真实文本中不可避免地会包含一定数量的非汉字字符，本文所说的“字”，也包括外文字母、阿拉伯数字和标点符号等字符。所有这些字符都是构词的基本单元。当然，汉字依然是这个单元集合中数量最多的一类字符。</p><p>把分词过程视为字的标注问题的一个重要优势在于，它能够平衡地看待词表词和未登录词的识别问题。</p><p>在这种分词技术中，文本中的词表词和未登录词都是用统一的字标注过程来实现的。在学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词(如人名、地名、机构名)识别模块。这使得分词系统的设计大大简化。在字标注过程中，所有的字根据预定义的特征进行词位特性的学习，获得一个概率模型。然后，在待分字串上，根据字与字之间的结合紧密程度，得到一个词位的标注结果。最后，根据词位定义直接获得最终的分词结果。总而言之，在这样一个分词过程中，分词成为字重组的简单过程。在学习构架上，由于可以不必特意强调词表词的信息，也不必专门设计针对未登录词的特定模块，这样使分词系统的设计变得尤为简单。</p><p>2001年Lafferty在最大熵模型（MEM）和隐马尔科夫模型（HMM）的基础上提出来了一种无向图模型–条件随机场（CRF）模型，它能在给定需要标记的观察序列的条件下，最大程度提高标记序列的联合概率。常用于切分和标注序列化数据的统计模型。</p><h3 id="基于统计分词方法的实现"><a href="#基于统计分词方法的实现" class="headerlink" title="基于统计分词方法的实现"></a>基于统计分词方法的实现</h3><p>现在，我们已经从全概率公式引入了语言模型，那么真正用起来如何用呢？ </p><p>我们有了统计语言模型，下一步要做的就是划分句子求出概率最高的分词，也就是对句子进行划分，最原始直接的方式，就是对句子的所有可能的分词方式进行遍历然后求出概率最高的分词组合。但是这种穷举法显而易见非常耗费性能，所以我们要想办法用别的方式达到目的。</p><p>仔细思考一下，假如我们把每一个字当做一个节点，每两个字之间的连线看做边的话，对于句子“中国人民银行”，我们可以构造一个如下的分词结构：</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180528NLP-01.jpg" alt="高山仰止"></p><p>我们要找概率最大的分词结构的话，可以看做是一个动态规划问题， 也就是说，要找整个句子的最大概率结构，对于其子串也应该是最大概率的。</p><p>对于句子任意一个位置t上的字，我们要从词典中找到其所有可能的词组形式，如上图中的第一个字，可能有：中、中国、中国人三种组合，第四个字可能只有民，经过整理，我们的分词结构可以转换成以下的有向图模型: </p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180528NLP-02.jpg" alt="高山仰止"></p><p>我们要做的就是找到一个概率最大的路径即可。我们假设Ct(k)表示第t个字的位置可能的词是k，那么可以写出状态转移方程： </p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180528NLP-03.jpg" alt="高山仰止"></p><p>其中k是当前位置的可能单词，l是上一个位置的可能单词，M是l可能的取值，有了状态转移返程，写出递归的动态规划代码就很容易了（这个方程其实就是著名的viterbi算法，通常在隐马尔科夫模型中应用较多）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;目前中文分词三大主流分词方法：基于词典的方法、基于规则的方法和基于统计的方法&lt;/p&gt;
&lt;h3 id=&quot;基于词典的方法&quot;&gt;&lt;a href=&quot;#基于词典的方法&quot; class=&quot;headerlink&quot; title=&quot;基于词典的方法&quot;&gt;&lt;/a&gt;基于词典的方法&lt;/h3&gt;&lt;figure
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Spark PipeLine</title>
    <link href="http://yoursite.com/2018/04/06/20180406Spark-PipeLine/"/>
    <id>http://yoursite.com/2018/04/06/20180406Spark-PipeLine/</id>
    <published>2018-04-06T07:08:45.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>以前spark-streaming用的比较多，ML库用的比较少，对pipeline之类的概念理解的不够深入。趁清明假期总结一下，温故知新。</p><p>spark提供的标准的机器学习算法能够将不同的算法和组件组合在一起，形成一个管道或者工作流。可以参考代码来看：<br><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180406PIPELINE01.jpg" alt="高山仰止"></p><p>可以看到pipeline可以顾名思义地理解为管道，将所有的算法和配置组建起来。MLlib对机器学习算法的API进行了标准化，使得将多种算法合并成一个pipeline或工作流变得更加容易。Pipeline的概念主要是受scikit-learn启发。接下来解释一下各个细节：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;以前spark-streaming用的比较多，ML库用的比较少，对pipeline之类的概念理解的不够深入。趁清明假期总结一下，温故知新。&lt;/p&gt;
&lt;p&gt;spark提供的标准的机器学习算法能够将不同的算法和组件组合在一起，形成一个管道或者工作流。可以参考代码来看：&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="Coding" scheme="http://yoursite.com/categories/Coding/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>word2vec学习问题简记</title>
    <link href="http://yoursite.com/2018/03/21/20180321word2vec%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/03/21/20180321word2vec学习笔记/</id>
    <published>2018-03-21T10:24:15.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>为了表示对前辈的尊敬，  <strong><a href="https://www.zybuluo.com/Dounm/note/591752" target="_blank" rel="noopener">这篇文章</a></strong>建议所有做word2vector的人都应该拜读</p><p>关于模型的解释，很简单：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">词 =&gt; 上下文  Skip-gram</span><br><span class="line">上下文 =&gt; 词  CBOW Continuous Bag-of-words Model</span><br></pre></td></tr></table></figure><p>损失函数是L =Σ𝑙𝑜𝑔𝑝(𝑤|𝐶𝑜𝑛𝑡𝑒𝑥𝑡(𝑤)),   w𝜖𝐶， 所以word2vec的关键问题：如何构造概率函数𝑝(𝑤|𝐶𝑜𝑛𝑡𝑒𝑥𝑡(𝑤))。实际上word2vector的损失函数用的是交叉熵。交叉熵是什么意思？这个要回归到香农熵定义当中去。</p><p>现有关于样本集的2个概率分布p和q，其中p为真实分布，q非真实分布。按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：H(p) = Σp(i) <em> log(1/p(i))。如果使用错误分布q来表示来自真实分布p的平均编码长度，则应该是：H(p,q) = Σp(i) </em> log(1/q(i))。因为用q来编码的样本来自分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。</p><p>也就是说，交叉熵越来越小的时候，错误分布就约逼近正确分布。</p><h3 id="基于-Hierarchical-Softmax-的模型"><a href="#基于-Hierarchical-Softmax-的模型" class="headerlink" title="基于 Hierarchical Softmax 的模型"></a>基于 Hierarchical Softmax 的模型</h3><p>也就是分层softmax，它的提出是为了应对传统的神经网络语言模型最后输出层的计算复杂度的问题，由于传统的softmax分母在每一次训练完一个单词后，需要更新训练词典里面的所有单词，这个复杂度是无法忍受的，所以我们换了一种衡量概率的方式，用二叉树路径连乘来表示最后的概率，从以前训练词语的权值矩阵，转化到训练二叉树的路径权重。</p><h4 id="简单展开CBOW"><a href="#简单展开CBOW" class="headerlink" title="简单展开CBOW"></a>简单展开CBOW</h4><p>假设：</p><ul><li>Context(w)由单词w的前后各c个词构成。</li><li>(Context(w)，w)为训练语料中的一个训练样例</li></ul><p>三个分层为：输入层，投影层，输出层</p><ul><li>输入层： 为Context(w)中的2c个词的词向量𝑣 (𝐶ontext(𝑤) 1) , 𝑣 (𝐶ontext( 𝑤 )2) , …𝑣 (𝐶ontext( 𝑤 )2𝑐−1) , 𝑣(𝐶ontext(𝑤)2𝑐)</li><li>投影层： 将输入层的2c个向量做求和，即Σ𝑣(𝐶𝑜𝑛𝑡𝑒𝑥𝑡(𝑤)𝑖)</li><li>输出层： 输出层对应一棵二叉树，它是以语料中出现过的词当叶子结点，以各词在语料中出现的次数当权值构造出的Huffman树。</li></ul><p>具体的详细过程可以看<a href="http://hlt.suda.edu.cn/~xwang/slides/word2vector.pdf" target="_blank" rel="noopener">这个连接</a>，实际上还是根据概率的大小的来做预测，只不过概率模型做了深思熟虑的选择， 换句话说所有的学习都是建立在概率模型选择的基础之上的。而关于这个过程有几个问题要考虑：</p><ul><li>输入层的词向量是怎么来的？</li><li>最后公式中为什么要用样本词向量的”和”乘以上文的词向量？</li></ul><p>第一个问题的答案是我们认为初始化的，大多采用uniform分布来参考。第二个问题的答案是神经网络中所有的都是：线性变换+非线性，这样才能拟合任意函数，这是一方面，另一方面，这么做正是word2vec优化的地方所在，传统的神经网络语言模型是将所有的词向量组合成矩阵，每次训练都有大纬度的矩阵参与训练，这样太复杂了。另外一个优点是，舍弃了神经网络中的隐藏层。</p><h4 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h4><p>目的跟上文的CBOW相反，但是步骤是一样的，但是SG的复杂度更高了，因为它要预测所有上下文的词语，多了一层循环，即一对多。详细过程要知道如何推导。</p><h3 id="基于Negative-Sampling的模型"><a href="#基于Negative-Sampling的模型" class="headerlink" title="基于Negative Sampling的模型"></a>基于Negative Sampling的模型</h3><p>所以我一直搞不明白什么叫做Negative Sampling，就是因为取了负样本  而负样本的作用是什么？博客上说NS模型可以提高训练速度，并且改善词向量的质量。</p><p>实际上，Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。</p><p>hierarchical softmax：本质是把 N 分类问题变成 log(N)次二分类</p><p>negative sampling：本质是预测总体类别的一个子集</p><p>那么针对标题开头的问题，NS 模型的本质，就是将训练集缩小到一个范围内，在这个范围内，目标词w作为正样本，其他词语作为负样本，而负样本的选择也有讲究，它摒弃了生冷词汇，按照单词出现的频率进行样本选择。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们要做的一切都是要求似然函数的最值，而所有的关键都在于构造合适的概率函数，那么我们思考选择什么样的模型比较合适呢？考虑到概率函数都是前向相乘的过程，所以我们要让那些频率高的词汇尽可能的排在前面，这样的话就可以避免掉很多不必要的乘法次数，所以Huffman模型能够很好的满足我们的要求。模型确定了以后剩下的就是数学的事情了。但是这么理解不对，Huffman树只是输出层的问题，不是训练，训练层中我们通过叠加向量+去掉隐藏层，达到了简化的目的，而输出层我们是为了规避softmax大量计算才采用的Huffman树，而不是训练方式。</p><p>还有一点就是词在Huffman树中都是以叶子节点的形式存在。</p><p>而有些同学不理解实际上它的训练过程是什么样子的，确实网络上的博客大部分都是在讲原理而没有讲落，<a href="https://iksinc.online/tag/continuous-bag-of-words-cbow/" target="_blank" rel="noopener">这篇文章</a>能够生动地讲述如何训练的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为了表示对前辈的尊敬，  &lt;strong&gt;&lt;a href=&quot;https://www.zybuluo.com/Dounm/note/591752&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这篇文章&lt;/a&gt;&lt;/strong&gt;建议所有做word2vector
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文技巧</title>
    <link href="http://yoursite.com/2018/03/21/20180321%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E6%8A%80%E5%B7%A7/"/>
    <id>http://yoursite.com/2018/03/21/20180321阅读论文技巧/</id>
    <published>2018-03-21T08:43:30.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>本文是<a href="http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf" target="_blank" rel="noopener">How to read a paper</a>的简洁笔记</p><h3 id="三步走战略"><a href="#三步走战略" class="headerlink" title="三步走战略"></a>三步走战略</h3><p>不要从头到尾的啃一篇论文，要有策略：</p><ul><li>粗读获取主旨</li><li>通读掌握内容</li><li>精读深度理解</li></ul><p>本文的最重要的观念在于，利用三段法阅读学术论文，每个阶段阅读都有其特殊的目的，且每一个目的都是以上一个阶段的结果为基础。</p><h4 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h4><p>第一阶段以快速扫描整篇文章，并获悉整篇论文的架构。也可以借此决定是否进行下一段的阅读，这个阶段大概需要5~10分钟，并且要严格遵循以下步骤。</p><ul><li><font color="red"><strong>仔细地</strong></font>阅读<font color="navy">标题，大纲与前言</font></li><li>阅读文章中的<font color="red">章节标题</font>，但不需要理会其中内容</li><li>阅读<font color="red">结论</font></li><li>快速扫过参考资料，并在心中勾选过阅读过的。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我觉得第四点还是算了，因为大部分人并不是做研究的，核心目的还是掌握文章主旨为上。</span><br></pre></td></tr></table></figure><p>第一阶段完成后，<font color="red">必须有能力回答以下几个问题</font>：</p><ul><li>论文属性：本篇论文属于那种类型？是量化测量类型的研究？还是新算法的提出？还是综述？</li><li>研究背景：哪一篇文章与本文有关？本文是基于那种理论进行研究的？</li><li>贡献：本文提出了什么新的观点？解决了什么问题？</li><li>清晰度：该论文的文笔是否清晰</li></ul><p>根据以上几点，如果你在5分钟内还不知道这篇论文在讲什么，那这篇论文不必要再看了。</p><h4 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h4><p>此阶段将更关注论文本身的内容，但是仍然忽略一些细节，如证明过程等等。这样做能够帮助你略记论文的关键处，或是加一些标注。但是还有几下几点需要特别注意的：</p><ul><li>请仔细阅读论文中的示意图，分析图，或是其他图表类型的内容。</li><li>请记得画哪些没有读到的有关参考资料，为将来进一步阅读做准备</li></ul><p>这段将耗费你大约一个小时，然后你可以领略改论文的内容。你应该有能力总结论文的主要理论，所支持的验证以及一些其他内容。有时候你无法在第二阶段结束后了解论文。这有可能是主题使用的不熟悉或是缩写，对你而言是新的，或是论文作者使用到证明和实验技巧是你无法理解，以至于你无法意会。或者是你本身太累，不在状态。此时你有三个选择：</p><ul><li>先把论文摆在一遍，再也不看了</li><li>回到论文本身，之后再继续涉猎相关的背景或者素材</li><li>进行第三阶段的阅读</li></ul><h4 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h4><p>为了彻底了解论文讲的是什么，特别是当一位期刊审查人时候，这一阶段就显得尤为重要了。第三阶段阅读的关键在于企图将论文实际重新验证一次，这个过程需要特别重视文章中的细枝末节。这个过程的耗时过程跟经验息息相关，阅读完后，应该可以凭借对论文的印象，勾勒出该论文的架构，并且可以指出论文的优缺点。特别是你应该有能力指出该论文中隐含的假说，遗漏的相关文献。如果是财务领域，应该联想到相关的类似的统计或者计量技术。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文是&lt;a href=&quot;http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;How to read a pa
      
    
    </summary>
    
      <category term="Muse" scheme="http://yoursite.com/categories/Muse/"/>
    
    
      <category term="Skills" scheme="http://yoursite.com/tags/Skills/"/>
    
  </entry>
  
  <entry>
    <title>A股游资整理</title>
    <link href="http://yoursite.com/2018/03/21/20180321A%E8%82%A1%E6%B8%B8%E8%B5%84%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2018/03/21/20180321A股游资整理/</id>
    <published>2018-03-21T08:35:41.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Huffman算法</title>
    <link href="http://yoursite.com/2018/03/21/20180321Huffman%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2018/03/21/20180321Huffman算法/</id>
    <published>2018-03-21T08:19:13.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>Huffman编码首先要构建Huffman树，根据频次的由小到大排列元素数据，构建二叉树，这样就能保证权重大的（频次高）的元素，会靠近树根，这样的话频次高的路径编码（左0右1）就短。</p><p>解释的比较好的文章请<a href="http://blog.csdn.net/FX677588/article/details/70767446" target="_blank" rel="noopener">点击这里</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Huffman编码首先要构建Huffman树，根据频次的由小到大排列元素数据，构建二叉树，这样就能保证权重大的（频次高）的元素，会靠近树根，这样的话频次高的路径编码（左0右1）就短。&lt;/p&gt;
&lt;p&gt;解释的比较好的文章请&lt;a href=&quot;http://blog.csdn.ne
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Logistic Regression的思考</title>
    <link href="http://yoursite.com/2018/03/21/20180321Logistic-Regression%E7%9A%84%E6%80%9D%E8%80%83/"/>
    <id>http://yoursite.com/2018/03/21/20180321Logistic-Regression的思考/</id>
    <published>2018-03-21T07:03:58.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>Spark Mllib 在做LR的时候，官方demo里给出了两个输出一个是coefficient 还有一个是intercept 那么这两个变量是什么意思？</p><p>我们回顾一下LR模型：hθ(x)=1/1+e(−θTx)</p><p>θTx 就是 θx + b  coefficient就是系数矩阵，而intercept就是截距的意思，就是这个b</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Spark Mllib 在做LR的时候，官方demo里给出了两个输出一个是coefficient 还有一个是intercept 那么这两个变量是什么意思？&lt;/p&gt;
&lt;p&gt;我们回顾一下LR模型：hθ(x)=1/1+e(−θTx)&lt;/p&gt;
&lt;p&gt;θTx 就是 θx + b  c
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>聊聊债券收益率</title>
    <link href="http://yoursite.com/2018/03/18/20180318%E8%81%8A%E8%81%8A%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87/"/>
    <id>http://yoursite.com/2018/03/18/20180318聊聊债券收益率/</id>
    <published>2018-03-18T15:17:46.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>美国克林顿政府时期的总统政策顾问James Carville曾经讲过一句话：</p><p>如果有来生，我曾经希望当美国总统、罗马教皇或棒球高分击球员，但现在，我只想托生在债券市场，因为它可以吓唬住任何人（I used to think if there was reincarnation， I wanted to come back as the President or the Pope or a .400 baseball hitter. But now，I want to come back as the bond market. You can intimidate everybody.）！</p><p>绝大多数人对这句话无感。</p><p>在大家的印象中，总觉得债券这玩意儿，固定收益的东西，乏味得很，哪有股票、期货和外汇市场的波动来得精彩刺激、激动人心，更是很少关心债券市场的情况……</p><p>然而，随着这么多年对金融市场了解得越来越深，我对这句话却愈加深有感触，而且发现自己以往认知的肤浅——特别是，我们用定期存款的思维去理解债券，这实在太低估了债券市场的波动和影响，甚至会造成完全错误的理解。</p><p>比方说，2011年和2013年，中国评级机构大公国际先后两次调低美国本、外币的主权信用等级，从A+调整到A，然后再到A-。当时很多爱国人士开始大声叫好，美国主权评级越低，那么美国国债收益率越高，这对目前大量持有美债的中国有利……</p><p>　　听见这种话，如果真爱国而且懂金融的人，恨不得暴揍这些蠢货一顿！</p><p>　　第一，所有已出售国债，其票面利率是保持不变的，美国国债收益率再涨，美国财政部也不会给持有债券者多付一分钱利息；</p><p>　　第二，信用评级下调，国债收益率上涨，意味着国债本身价格的下跌——100元面值的国债售价98，意味着收益率=（100-98）/98=2.04%；如果售价95，意味着收益率=（100-95）/95=5.25%，所以如果收益率暴涨，就是国债价格暴跌，对中国持有的美国国债只有坏处没有好处！</p><p>　　最基本也应该知道：</p><p>　　国债收益率上涨===国债价格下跌！</p><p>　　2010年面临破产之时，希腊国债收益率高达1000%，这意味着，面值100欧元的希腊国债，在市场上售价连10元都不到，你居然还说持有希腊国债是好事，你说这些蠢货们的金融知识该有多匮乏！</p><p>　　实际上，你经常在财经媒体中听到的那个“债券收益率”，其得来就是因为债券价格的变动而计算得出——如果某个债券收益率暴涨，常常意味着这个债券偿付能力有问题，价格自然就是暴跌啊！</p><p>　　有人该疑惑了，国债收益率不是国债发售的时候就已经确定了么？</p><p>　　嘿嘿，你问到点上了，债券收益率，就是个金融知识的深坑啊！</p><p>　　以国债为例，一般来说有5种收益率。<br>　　<br>　　1）名义收益率：</p><p>　　通常称之为票面利率，这是国债一开始发行就确定的收益率。</p><p>　　例如：某10年期中国国债，面值100元发售，每年支付一次利息，持有人每年某个固定日期都会从财政部收到5元钱的收益，这就意味着名义收益率5%。</p><p>　　2）即期收益率：意思是按照现在的国债价格和票面收益算出来的收益率。</p><p>　　例如：某十年期中国国债，面值100元，票面利率5%，而当前市场价格96元，那么，即期收益率=5/96=5.2%。</p><p>　　3）到期收益率：这个稍复杂一点儿，要考虑当前债券价格和持有到期后财政部返还所有本息的价差和距离到期的剩余年份。</p><p>　　例如：某十年期中国国债，面值100元，票面利率5%，当前市场上价格96元，还有8年到期，那么，到期收益率=[5+（100-96）/8]/96=5.73%。</p><p>　　<font color="navy"><strong>同学们，敲黑板了，我们通常在财经新闻中听到的那个“国债收益率”或“债券收益率”，通常情况下就是指“到期收益率”！</strong></font></p><p>　　4）认购者收益率：在绝大多数时候，100元面值的国债，财政部是不会按照100元来卖的，而是要根据当前市场上的国债收益率价格进行调整，还要根据各购买国债的金融机构投标情况来确定价格。这个收益率，指的是认购者买国债之后，持有到期的收益率。这需要考虑财政部出售债券价格、本来的票面收益率等因素。</p><p>　　例如：某十年期中国国债，面值100元，票面利率5%，购买者从财政部购买的价格是102元，那么，认购者收益率=[5+（100-102）/10]/102=4.71%。</p><p>　　<font color="navy"><strong>再次敲黑板了，这个收益率，就是通常所谓的“国债发行利率”</strong></font>。</p><p>　　5）持有期收益率：国债价格本身有涨跌，有人就会想着在涨涨跌跌中赚差价，而这个收益率就是在你赚取差价时候的收益率。</p><p>　　例如：某十年期中国国债，面值100元，票面利率5%，每年付息一次。我以96元在年初买进，除了获得国债利息收入外，我还预计2年后会涨到102元，并在那时卖出，这样一来，持有期收益率=[5+（102-96）/2]/96=8.33%。</p><p>　　<font color="navy"><strong>还要敲黑板，这个收益率，就是投资者买卖债券所获取的投资收益率</strong></font>。</p><p>　　债券收益率的概念，基本就这点儿知识了。</p><p>　　很多人可能不知道，美国债券市场远比股票市场的规模大。</p><p>　　仅美国国债市场，2017年年中其规模已经达到19.8万亿美元，如果进一步加上公司债、市政债（地方债）、联邦机构债、公司债以及房地产抵押证券（MBS）和资产支持证券（ABS），总规模超过40万亿美元。</p><p>　　相比之下，在全世界投资者心目中这么牛逼的纽约交易所，目前其股票总市值也不过20万亿美元左右，加上纳斯达克的9万亿，总额也就30万亿美元的样子。</p><p>　　那——为什么我们都知道美国股市，却很少人听说过美国债市。</p><p>　　答案并不奇怪——</p><p>　　因为，债市交易者绝大部分都是诸如银行、保险公司、投资银行（券商）、信托公司、大型对冲基金这样的机构用户，乃至各国政府（美元国债很活跃的购买对象），却极少个人投资者在债市里折腾。</p><p>　　但你想想嘛，真正决定市场运行的，是机构还是散户？</p><p>　　正是因为都是大手笔的机构投资者在市场上折腾，所以一旦债市收益率出现较大波动，常常能够在整个金融市场掀起滔天巨浪，相比之下，股市的波动反而影响没有那么大。</p><p>　　<font color="red"><strong>在一个市场经济国家里，债券市场才是资本市场的核心，而十年期国债收益率则是金融市场上资产定价的锚，我称之为“定海神针”，无论债券、股票、期货还是房地产，其价格都会受到“定海神针”的影响</strong></font>。</p><p>　　不考虑通胀的话，影响方向很确定：</p><p>　　定海神针下移，资产价格上升；</p><p>　　定海神针上移，资产价格下跌。</p><p>　　为什么会这样呢？</p><p>　　因为有中央政府和央妈印钞作保证，10年期国债的收益率通常被称为“无风险收益率”——也就是说，购买10年期国债，我的收益率是不用承担风险的；</p><p>　　与无风险收益率相对，一个人如果购买非国债的其他债券、股票、房子等资产，他需要承担价格涨跌风险，当然也要求获得比国债更高的收益率，所以嘛——</p><p>　　10年期国债收益率，就成为了各类资产收益率的铁底。</p><p>　　如果国债收益率升高，将意味着债券收益率、股票的收益率（市盈率的倒数）、期货升贴水率、房子收益率（租金房价比）的底部都要抬高，这一抬高不要紧，在同样收益（债券票面收益、公司股票分红、房产租金等）的情况下，将意味着债券、股票、房子的价格的下跌（原理与前面讲债券价格下跌一样）……</p><p>　　否则的话，如果这些资产的收益率比无风险利率还要低，金融机构肯定选择卖出股票、卖出房子、卖出债券——归根结底还是会导致资产价格下跌。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;美国克林顿政府时期的总统政策顾问James Carville曾经讲过一句话：&lt;/p&gt;
&lt;p&gt;如果有来生，我曾经希望当美国总统、罗马教皇或棒球高分击球员，但现在，我只想托生在债券市场，因为它可以吓唬住任何人（I used to think if there was reinc
      
    
    </summary>
    
      <category term="Muse" scheme="http://yoursite.com/categories/Muse/"/>
    
    
      <category term="Investment" scheme="http://yoursite.com/tags/Investment/"/>
    
  </entry>
  
  <entry>
    <title>最近投资理念的一些思考</title>
    <link href="http://yoursite.com/2018/03/17/20180317%E6%9C%80%E8%BF%91%E6%8A%95%E8%B5%84%E7%90%86%E5%BF%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"/>
    <id>http://yoursite.com/2018/03/17/20180317最近投资理念的一些思考/</id>
    <published>2018-03-17T14:18:04.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>2017年上半年的时候，上证50并不是获得很多人的认可，周阳老师说了一句话，你敢看空上证50？</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180317touzi01.jpg" alt="高山仰止"></p><p>彼时的疯狂都在围绕着“千年大计”展开。与安邦的龙江通话，讲起了华夏幸福的事情，持续买几年的华夏幸福也是很有本事的一件事情。如果我们彼时买入50的话，现在的收益是多少不在话下，看图说话。2018年出因为加息刺激和量化算法导致美股的崩盘，情绪传到到中国来，自己没有应对这种系统系风险的经验，应该在第二天就采取应对措施的，A股市场的上的反应也比较迟钝，在盘后看到了葛卫东清仓，才有了一些恐惧，后来自己的股票遭到腰斩，同时自己又不自信的清仓，错上加错，形成双杀。</p><p>总体来讲，未来中国的国运处于上升阶段，这届领导班子的政绩也都看到了 ，钢哥也说了，未来一定是慢牛的。纵观我们的一生，就像指数一样，一些我们当时认为迈不过去的坎坷，事后看去都是风淡云轻。所以人在遇到了坎坷，投资也好，人生也好，一定要把时间轴拉长了看，这样的话就会高屋建瓴。这一点是要深信不疑的。</p><p>就投资再多说两句，长牛的意义不仅限于上证指数的慢牛，看看美国的股指：</p><ul><li>道琼斯：目前由30家巨头公司组成，类似于我们的上证50</li><li>纳斯达克：主要是科技股，类似我们的创业板</li><li>标准普尔：股票的大杂烩，类似于沪深300，是美国经济的整体情况</li></ul><p>如果对标美国股指的话，我们的车才刚刚启动，而18年创业板才刚刚启动，前途一片光明。但是前途是光明的，道路是曲折的，美元已经渐渐有了加息的趋势了，存量资金在A股市场上的换道行驶必然会让别的股票受损。所以中国能不能走出来自己的道路，人民币能不能摆脱美元的束缚，天佑我中华了。所以言归正传，未来的上证我还是继续看好，<font color="navy"><strong>而创业板科技股也要保持很好的研究，因为新经济的刺激需要的是创新而不是守成</strong></font>。</p><p>为了不在文中班门弄斧，这里将任泽平先生17年末的讲话，以自己的观点和角度，做一个总结：</p><p>中国要增速换挡，08年以前的房产刺激不会再出现了，房地产的投资高潮已经过去了，未来房地产行业会向着行业集中度发展。</p><p>一轮产能周期分为四个阶段，第一个阶段就是在经济繁荣的时候，企业家过度乐观，认为做出决策的能力高人一筹，这时候进行大规模的产能扩张，随后引发产能过剩；进入第二个阶段，产能过剩以后，供需格局恶化，中小企业退出，淘汰落后产能出清；进入第三个阶段，产能出清的尾声，行业集中度提升，剩者为王，企业利润改善，银行不良率下降，修复资产负债表，为新一轮产能扩张积蓄力量；到了第四个阶段，持续的盈利改善和资产负债表修复，最终将会迎来新一轮的产能扩张。</p><p>房地产：长期看人口，中期看土地，短期看金融。</p><p>三月份任先生的讲座还需要做以总结，最近创业板的事情，印证了钢哥说的一点，就是股价的波动就是人心的波动，正所谓“圣人无心，以众人之心为心”，独角兽和创业板的情绪都没有把握住，可以说是对人心的不理解，也可以说是自己的不自信。本来就是个博弈的过程，希望自己未来能够更决断一点。但是，风控仍然是第一位的。想获得高收益，高风险博弈是少不掉的。</p><p>同时，在看了任博士的调研精神以及调研深度上，自己的钻研和调查积累还差的远，离周阳老师那个熟练度也差的远，我觉得价值投资有一个方面是人性懒惰的体现。对于A股市场上的股票，我目前建议按照数据库样式自己整理一份出来，同时加上板块叠加字段（其实这种操作已经有人做了，类似于各大财经软件上的条件选股，但是我这个远没有他们的复杂，我仅仅是对整个市场的概念有一个大体的了解），具体的操作同花顺已经可以按照星级下载了，剩下的自行整理，日进一步。</p><p>18年年初的时候，资金会有个前辈说严谨就不会亏钱，这个严谨二字如何定义，还需斟酌。或许针对的就是目前我这种在雪球上抄作业的人吧。亏损的时候大家都坐不住，投资真的需要信念支撑，快钱也要学会赚。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2017年上半年的时候，上证50并不是获得很多人的认可，周阳老师说了一句话，你敢看空上证50？&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20
      
    
    </summary>
    
      <category term="Muse" scheme="http://yoursite.com/categories/Muse/"/>
    
    
      <category term="Investment" scheme="http://yoursite.com/tags/Investment/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot始于足下</title>
    <link href="http://yoursite.com/2018/03/15/20180315SpringBoot%E5%A7%8B%E4%BA%8E%E8%B6%B3%E4%B8%8B/"/>
    <id>http://yoursite.com/2018/03/15/20180315SpringBoot始于足下/</id>
    <published>2018-03-15T09:00:59.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>以前我们做Spring的时候，无论项目大小，都要经历以下过程：</p><ul><li>配置web.xml，加载spring和spring mvc</li><li>配置数据库连接、配置spring事务</li><li>配置加载配置文件的读取，开启注解</li><li>配置日志文件</li></ul><p>…</p><p>配置完成之后部署tomcat 调试</p><p>…</p><p>无论项目大小，我们都要经历上面复杂的项目流程。</p><p>那么我们用了Spring Boot的时候，情况就会改善很多。Spring的入门可以根据IBM和</p><p>老生常谈的MVC，温故知新：</p><p>view：视图。这个很容易理解，其实view层就是用户用户可以看到的东西。后台怎么处理不关心，只关心怎么样想用户展示信息。</p><p> controller：也可以成为action层，业务模块流程。我经常喜欢用控制视图的跳转来简单形容，但是这个是不全面的，因为他除了控制视图的转换之外，还控制了业务的逻辑，但是，这里的控制业务逻辑不是业务逻辑的实现，而仅仅是一个大的模块，你看到之后，知道它实现了这个业务逻辑，但是怎么实现的，不需要关心，仅仅需要调用service层里的一个方法即可，这样使controller层看起来更加清晰。</p><p>service：业务逻辑层。接着controller层中，可以想到，service层是业务逻辑（商务逻辑）的具体实现。它向上层的controller层提供接口，并且使用dao层提供的接口。存在的必要性：有时候，我认为更多的时刻，service层中仅仅是调用dao层中的一个方法，那么它是否有必要存在呢？答案是肯定的。因为，假如将来客户的业务有一定的变动，那么这样一来，你只需要在service层中进行一些变动即可。记住，你写程序不应该仅仅为实现功能考虑，更多的还是应该为将来的维护考虑，因为大部分的时间还是在维护上的。</p><p>dao：数据访问对象。也就是我们经常说的数据持久层，负责与数据库进行联络的一些任务都封装在此，DAO层的设计首先是设计DAO的接口，然后在Spring的配置文件中定义此接口的实现类，然后就可在模块中调用此接口来进行数据业务的处理，而不用关心此接口的具体实现类是哪个类，显得结构非常清晰，DAO层的数据源配置，以及有关数据库连接的参数都在Spring的配置文件中进行配置。 </p><p><code>这些层次的理解还需要结合实际开发后来感受一下</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;以前我们做Spring的时候，无论项目大小，都要经历以下过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置web.xml，加载spring和spring mvc&lt;/li&gt;
&lt;li&gt;配置数据库连接、配置spring事务&lt;/li&gt;
&lt;li&gt;配置加载配置文件的读取，开启注解&lt;/li&gt;
&lt;li
      
    
    </summary>
    
      <category term="Coding" scheme="http://yoursite.com/categories/Coding/"/>
    
    
      <category term="Spring" scheme="http://yoursite.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降算法的三种形式：BGD, SGD, MBGD</title>
    <link href="http://yoursite.com/2018/03/13/20180313%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%89%E7%A7%8D%E5%BD%A2%E5%BC%8F%EF%BC%9ABGD%EF%BC%8CSGD-MBGD/"/>
    <id>http://yoursite.com/2018/03/13/20180313梯度下降算法的三种形式：BGD，SGD-MBGD/</id>
    <published>2018-03-13T07:27:10.000Z</published>
    <updated>2018-07-06T17:24:34.159Z</updated>
    
    <content type="html"><![CDATA[<p>在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。</p><h4 id="BGD-批量梯度下降算法"><a href="#BGD-批量梯度下降算法" class="headerlink" title="BGD 批量梯度下降算法"></a>BGD 批量梯度下降算法</h4><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180313gradient01.jpg" alt="高山仰止"></p><p>首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。</p><p>从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p><h5 id="梯度下降的相关概念"><a href="#梯度下降的相关概念" class="headerlink" title="梯度下降的相关概念"></a>梯度下降的相关概念</h5><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180313gradient02.jpg" alt="高山仰止"></p><p>算法的具体过程可以见下图：</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180313gradient03.jpg" alt="高山仰止"></p><p>BGD最关键的地方在于理解，为什么梯度参数更新的时候用原参数 - 损失函数关于参数的偏导数。因为函数关于某一点的导数的意义在于这一点的变化率，那么关于参数的导数就是参数的变化率，而乘以步长的意义在于参数变化了多少。</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180313gradient04.jpg" alt="高山仰止"></p><p>那么我们可以看到，每更新一次参数就要用到所有的训练数据，这样的话是非常耗时的。</p><h4 id="SGD-Stochastic-Gradient-Descent-随机梯度下降算法"><a href="#SGD-Stochastic-Gradient-Descent-随机梯度下降算法" class="headerlink" title="SGD Stochastic Gradient Descent 随机梯度下降算法"></a>SGD Stochastic Gradient Descent 随机梯度下降算法</h4><p>由于批梯度下降每跟新一个参数的时候，要用到所有的样本数，所以训练速度会随着样本数量的增加而变得非常缓慢。随机梯度下降正是为了解决这个办法而提出的。它是利用每个样本的损失函数对θ求偏导得到对应的梯度，来更新θ：</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180313gradient05.jpg" alt="高山仰止"></p><p>也就是说在SGD中，算法不再考虑每个样本的意见了，而是见好就收，针对当前样本进行上文所述的修正。显而易见的，这种方法在速度上取得了绝对的优势，但是在收敛性上显得过于随意。随机梯度下降是通过每个样本来迭代更新一次，对比上面的批量梯度下降，迭代一次需要用到所有训练样本（往往如今真实问题训练数据都是非常巨大），一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p><h4 id="MBGD-Mini-batch-Gradient-Descent"><a href="#MBGD-Mini-batch-Gradient-Descent" class="headerlink" title="MBGD Mini-batch Gradient Descent"></a>MBGD Mini-batch Gradient Descent</h4><p>上面两种算法各有优缺点，而MBGD就是中和了上面两种方法的，也就是说我们更新参数的时候，能够自己选择使用样本的个数</p><p><img src="https://raw.githubusercontent.com/Hunglish/Blog-Photos/master/picture/20180313gradient06.jpg" alt="高山仰止"></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ol><li><p>批梯度下降每次更新使用了所有的训练数据，最小化损失函数，如果只有一个极小值，那么批梯度下降是考虑了训练集所有数据，是朝着最小值迭代运动的，但是缺点是如果样本值很大的话，更新速度会很慢。</p></li><li><p>随机梯度下降在每次更新的时候，只考虑了一个样本点，这样会大大加快训练数据，也恰好是批梯度下降的缺点，但是有可能由于训练数据的噪声点较多，那么每一次利用噪声点进行更新的过程中，就不一定是朝着极小值方向更新，但是由于更新多轮，整体方向还是大致朝着极小值方向更新，又提高了速度。</p></li><li><p>小批量梯度下降法是为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性综合而来，但是这里注意，不同问题的batch是不一样的，听师兄跟我说，我们nlp的parser训练部分batch一般就设置为10000，那么为什么是10000呢，我觉得这就和每一个问题中神经网络需要设置多少层，没有一个人能够准确答出，只能通过实验结果来进行超参数的调整。</p></li></ol><p><a href="https://zhuanlan.zhihu.com/p/25765735" target="_blank" rel="noopener">参考此文</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。&lt;/p&gt;
&lt;h4 id=&quot;BGD-批量梯度下降算法&quot;&gt;&lt;a href=&quot;#BGD-批量梯度下
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
</feed>
